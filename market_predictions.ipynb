{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "LrxWpwfrc2kI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjiEZursb-ls",
        "outputId": "f60ff877-b3de-4c53-9294-2664de6c5ac3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ta) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ta) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=f65ceb761bc7f6342d408a19f8b1510a815c948a06fca62db8cc5106ce06525f\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/d7/29/7781cc5eb9a3659d032d7d15bdd0f49d07d2b24fec29f44bc4\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLmBGlWKrYtF",
        "outputId": "eda0ddfc-5c74-45d0-915c-144e053c8040"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Uv7V5IiLtnnn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import ta\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLcDn7_qJVrz",
        "outputId": "bd95f8d3-cfbf-47f9-a59f-e33f391475b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"drive/My Drive/stockhack/\")"
      ],
      "metadata": {
        "id": "trMSni8pRdxF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "md3IJ4x4c7Xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import ta\n",
        "\n",
        "class StockPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.scalers: Dict[str, StandardScaler] = {}\n",
        "        self.stock_files = {\n",
        "            'CELH': 'celh_data.csv',\n",
        "            'CVNA': 'cvna_data.csv',\n",
        "            'UPST': 'upst_data.csv',\n",
        "            'ALT': 'alt_data.csv',\n",
        "            'FUBO': 'fubo_data.csv'\n",
        "        }\n",
        "\n",
        "    def load_all_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Load and combine all stock data.\"\"\"\n",
        "        all_data = []\n",
        "\n",
        "        for stock, file in self.stock_files.items():\n",
        "            df = pd.read_csv(file)\n",
        "            # Parse dates using DD/MM/YYYY format\n",
        "            df['Dates'] = pd.to_datetime(df['Dates'], format='%d/%m/%Y')\n",
        "            all_data.append(df)\n",
        "\n",
        "        combined_df = pd.concat(all_data, ignore_index=True)\n",
        "        return combined_df.sort_values(['Dates', 'Stock']).reset_index(drop=True)\n",
        "\n",
        "    def add_technical_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Add technical indicators and derived features.\"\"\"\n",
        "        df = df.drop(['Moving Average 20 Day.1'], axis=1, errors='ignore')\n",
        "\n",
        "        # Group by stock to calculate features for each stock separately\n",
        "        dfs = []\n",
        "\n",
        "        for stock in df['Stock'].unique():\n",
        "            stock_df = df[df['Stock'] == stock].copy()\n",
        "\n",
        "            # Sort by date\n",
        "            stock_df = stock_df.sort_values('Dates')\n",
        "\n",
        "            # Forward fill price data\n",
        "            price_cols = ['Last Price', 'Open Price', 'High Price', 'Low Price',\n",
        "                         'Bid Price', 'Ask Price', 'VWAP Price']\n",
        "            stock_df[price_cols] = stock_df[price_cols].ffill().bfill()\n",
        "\n",
        "            # Fill volume data with rolling median\n",
        "            vol_cols = ['Volume', 'Average Volume 30 Day', 'Turnover / Traded Value']\n",
        "            for col in vol_cols:\n",
        "                stock_df[col] = stock_df[col].fillna(stock_df[col].rolling(30, min_periods=1).median())\n",
        "\n",
        "            # Fill technical indicators with rolling median\n",
        "            tech_cols = ['RSI 14 Day', 'Moving Average 20 Day', 'Moving Average 50 Day',\n",
        "                        'Moving Average 200 Day', 'Volatility 30 Day', 'Current Market Cap']\n",
        "            for col in tech_cols:\n",
        "                stock_df[col] = stock_df[col].fillna(stock_df[col].rolling(20, min_periods=1).median())\n",
        "\n",
        "            # After filling NaN values, calculate technical indicators\n",
        "            close = stock_df['Last Price']\n",
        "            high = stock_df['High Price']\n",
        "            low = stock_df['Low Price']\n",
        "            volume = stock_df['Volume']\n",
        "\n",
        "            # Add momentum indicators\n",
        "            stock_df['ROC_5'] = ta.momentum.ROCIndicator(close, window=5).roc()\n",
        "            stock_df['ROC_10'] = ta.momentum.ROCIndicator(close, window=10).roc()\n",
        "            stock_df['ROC_20'] = ta.momentum.ROCIndicator(close, window=20).roc()\n",
        "\n",
        "            # Add MACD\n",
        "            macd = ta.trend.MACD(close)\n",
        "            stock_df['MACD'] = macd.macd()\n",
        "            stock_df['MACD_Signal'] = macd.macd_signal()\n",
        "            stock_df['MACD_Diff'] = macd.macd_diff()\n",
        "\n",
        "            # Add Bollinger Bands\n",
        "            bollinger = ta.volatility.BollingerBands(close)\n",
        "            stock_df['BB_High'] = bollinger.bollinger_hband()\n",
        "            stock_df['BB_Mid'] = bollinger.bollinger_mavg()\n",
        "            stock_df['BB_Low'] = bollinger.bollinger_lband()\n",
        "            stock_df['BB_Width'] = (stock_df['BB_High'] - stock_df['BB_Low']) / stock_df['BB_Mid']\n",
        "\n",
        "            # Add ATR\n",
        "            stock_df['ATR'] = ta.volatility.AverageTrueRange(high, low, close).average_true_range()\n",
        "\n",
        "            # Add volume indicators\n",
        "            stock_df['OBV'] = ta.volume.OnBalanceVolumeIndicator(close, volume).on_balance_volume()\n",
        "            stock_df['ADI'] = ta.volume.AccDistIndexIndicator(high, low, close, volume).acc_dist_index()\n",
        "\n",
        "            # Add price-derived features\n",
        "            stock_df['Daily_Return'] = close.pct_change()\n",
        "            stock_df['Log_Return'] = np.log(close/close.shift(1))\n",
        "            stock_df['High_Low_Range'] = (high - low) / close\n",
        "\n",
        "            # Add MA ratios\n",
        "            stock_df['Price_MA_Ratio_20'] = close / stock_df['Moving Average 20 Day']\n",
        "            stock_df['Price_MA_Ratio_50'] = close / stock_df['Moving Average 50 Day']\n",
        "            stock_df['Price_MA_Ratio_200'] = close / stock_df['Moving Average 200 Day']\n",
        "\n",
        "            # Add distance to MAs\n",
        "            stock_df['Distance_To_MA_20'] = (close - stock_df['Moving Average 20 Day']) / stock_df['Moving Average 20 Day']\n",
        "            stock_df['Distance_To_MA_50'] = (close - stock_df['Moving Average 50 Day']) / stock_df['Moving Average 50 Day']\n",
        "\n",
        "            # Add volume indicators\n",
        "            stock_df['Volume_MA_Ratio'] = volume / stock_df['Average Volume 30 Day']\n",
        "            stock_df['Volume_Price_Trend'] = (close.pct_change() * volume).rolling(5).mean()\n",
        "\n",
        "            # Fill any remaining NaN values with 0\n",
        "            stock_df = stock_df.fillna(0)\n",
        "\n",
        "            dfs.append(stock_df)\n",
        "\n",
        "        return pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    def prepare_data_for_modeling(self, df: pd.DataFrame, window_size: int = 252,  # One year of trading days\n",
        "                              val_size: int = 63,  # ~3 months\n",
        "                              test_size: int = 21,  # ~1 month\n",
        "                              target_days: int = 5) -> Tuple[Dict[str, np.ndarray], ...]:\n",
        "      \"\"\"\n",
        "      Prepare data for modeling using rolling windows approach.\n",
        "\n",
        "      Args:\n",
        "          df: Processed DataFrame\n",
        "          window_size: Number of days for training window\n",
        "          val_size: Number of days for validation\n",
        "          test_size: Number of days for testing\n",
        "          target_days: Number of days ahead to predict\n",
        "      \"\"\"\n",
        "      feature_cols = [\n",
        "          # Price features\n",
        "          'Last Price', 'Open Price', 'High Price', 'Low Price',\n",
        "          'Moving Average 20 Day', 'Moving Average 50 Day', 'Moving Average 200 Day',\n",
        "\n",
        "          # Volume features\n",
        "          'Volume', 'Average Volume 30 Day',\n",
        "\n",
        "          # Technical indicators\n",
        "          'RSI 14 Day', 'Volatility 30 Day',\n",
        "          'ROC_5', 'ROC_10', 'ROC_20',\n",
        "          'MACD', 'MACD_Signal', 'MACD_Diff',\n",
        "          'BB_High', 'BB_Mid', 'BB_Low', 'BB_Width', 'ATR',\n",
        "          'OBV', 'ADI',\n",
        "\n",
        "          # Derived features\n",
        "          'Daily_Return', 'Log_Return', 'High_Low_Range',\n",
        "          'Price_MA_Ratio_20', 'Price_MA_Ratio_50', 'Price_MA_Ratio_200',\n",
        "          'Distance_To_MA_20', 'Distance_To_MA_50',\n",
        "          'Volume_MA_Ratio', 'Volume_Price_Trend'\n",
        "      ]\n",
        "\n",
        "      X_train_dict = {}\n",
        "      X_val_dict = {}\n",
        "      X_test_dict = {}\n",
        "      y_train_dict = {}\n",
        "      y_val_dict = {}\n",
        "      y_test_dict = {}\n",
        "\n",
        "      for stock in df['Stock'].unique():\n",
        "          # Get data for this stock\n",
        "          stock_df = df[df['Stock'] == stock].copy()\n",
        "          stock_df = stock_df.sort_values('Dates')\n",
        "\n",
        "          # Create target variable (future returns)\n",
        "          stock_df['target'] = stock_df['Last Price'].shift(-target_days) / stock_df['Last Price'] - 1\n",
        "\n",
        "          # Get the last window of data\n",
        "          total_size = window_size + val_size + test_size\n",
        "          final_window = stock_df.iloc[-total_size:].copy()\n",
        "\n",
        "          # Split into train/val/test\n",
        "          train_data = final_window.iloc[:window_size]\n",
        "          val_data = final_window.iloc[window_size:window_size + val_size]\n",
        "          test_data = final_window.iloc[window_size + val_size:]\n",
        "\n",
        "          # Scale features using only training data\n",
        "          scaler = StandardScaler()\n",
        "          train_features = scaler.fit_transform(train_data[feature_cols])\n",
        "          val_features = scaler.transform(val_data[feature_cols])\n",
        "          test_features = scaler.transform(test_data[feature_cols])\n",
        "\n",
        "          # Store splits\n",
        "          X_train_dict[stock] = train_features\n",
        "          X_val_dict[stock] = val_features\n",
        "          X_test_dict[stock] = test_features\n",
        "\n",
        "          y_train_dict[stock] = train_data['target'].fillna(0).values\n",
        "          y_val_dict[stock] = val_data['target'].fillna(0).values\n",
        "          y_test_dict[stock] = test_data['target'].fillna(0).values\n",
        "\n",
        "          # Print info about the splits\n",
        "          print(f\"\\nSplit info for {stock}:\")\n",
        "          print(f\"Training period: {train_data['Dates'].iloc[0]} to {train_data['Dates'].iloc[-1]}\")\n",
        "          print(f\"Validation period: {val_data['Dates'].iloc[0]} to {val_data['Dates'].iloc[-1]}\")\n",
        "          print(f\"Test period: {test_data['Dates'].iloc[0]} to {test_data['Dates'].iloc[-1]}\")\n",
        "          print(f\"Training samples: {len(train_features)}\")\n",
        "          print(f\"Validation samples: {len(val_features)}\")\n",
        "          print(f\"Test samples: {len(test_features)}\")\n",
        "\n",
        "      return (X_train_dict, X_val_dict, X_test_dict,\n",
        "              y_train_dict, y_val_dict, y_test_dict)"
      ],
      "metadata": {
        "id": "HhPL0MdNcOc-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def load_and_process_data(data_files: Dict[str, str],\n",
        "                         short_data_path: str = None,\n",
        "                         target_horizon: int = 5) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load and process all data sources.\n",
        "\n",
        "    Args:\n",
        "        data_files: Dictionary mapping stock symbols to their data file paths\n",
        "        short_data_path: Path to short interest data (optional)\n",
        "        target_horizon: Number of days ahead to predict\n",
        "\n",
        "    Returns:\n",
        "        Processed DataFrame with all features\n",
        "    \"\"\"\n",
        "    # Initialize preprocessor\n",
        "    preprocessor = StockPreprocessor()\n",
        "\n",
        "    # Load and process the main data\n",
        "    processed_df = preprocessor.load_all_data()\n",
        "\n",
        "    # Add technical features\n",
        "    processed_df = preprocessor.add_technical_features(processed_df)\n",
        "\n",
        "    # Add short interest data if provided\n",
        "    if short_data_path:\n",
        "        short_data = pd.read_csv(short_data_path)\n",
        "        # Convert dates to datetime\n",
        "        short_data['Dates'] = pd.to_datetime(short_data['Dates'], format='%d/%m/%Y')\n",
        "\n",
        "        # Merge with main data\n",
        "        processed_df = pd.merge(processed_df, short_data,\n",
        "                              on=['Dates', 'Stock'],\n",
        "                              how='left')\n",
        "\n",
        "    return processed_df"
      ],
      "metadata": {
        "id": "n_EueDj_bhkp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_val_test_split(df: pd.DataFrame,\n",
        "                                window_size: int = 252,  # One year\n",
        "                                val_size: int = 63,      # ~3 months\n",
        "                                test_size: int = 21,     # ~1 month\n",
        "                                target_horizon: int = 5) -> Tuple[Dict[str, np.ndarray], ...]:\n",
        "    \"\"\"\n",
        "    Create train/validation/test splits for each stock.\n",
        "\n",
        "    Args:\n",
        "        df: Processed DataFrame with all features\n",
        "        window_size: Number of days for training window\n",
        "        val_size: Number of days for validation\n",
        "        test_size: Number of days for testing\n",
        "        target_horizon: Number of days ahead to predict\n",
        "\n",
        "    Returns:\n",
        "        Tuple of dictionaries containing train/val/test splits for each stock\n",
        "    \"\"\"\n",
        "    # Initialize preprocessor\n",
        "    preprocessor = StockPreprocessor()\n",
        "\n",
        "    # Prepare data for modeling using window sizes\n",
        "    return preprocessor.prepare_data_for_modeling(\n",
        "        df,\n",
        "        window_size=window_size,\n",
        "        val_size=val_size,\n",
        "        test_size=test_size,\n",
        "        target_days=target_horizon\n",
        "    )\n",
        "\n",
        "\n",
        "def test_data_pipeline():\n",
        "    \"\"\"Test the complete data pipeline.\"\"\"\n",
        "    print(\"Testing data pipeline...\")\n",
        "\n",
        "    # Define stock files\n",
        "    data_files = {\n",
        "        'CELH': 'celh_data.csv',\n",
        "        'CVNA': 'cvna_data.csv',\n",
        "        'UPST': 'upst_data.csv',\n",
        "        'ALT': 'alt_data.csv',\n",
        "        'FUBO': 'fubo_data.csv'\n",
        "    }\n",
        "\n",
        "    # Load and process data\n",
        "    print(\"\\nLoading and processing data...\")\n",
        "    processed_df = load_and_process_data(data_files)\n",
        "\n",
        "    print(f\"\\nProcessed data shape: {processed_df.shape}\")\n",
        "    print(\"\\nFeature columns:\")\n",
        "    print(processed_df.columns.tolist())\n",
        "\n",
        "    # Create splits\n",
        "    print(\"\\nCreating train/val/test splits...\")\n",
        "    splits = create_train_val_test_split(\n",
        "        processed_df,\n",
        "        window_size=252,  # One year of training\n",
        "        val_size=63,      # ~3 months of validation\n",
        "        test_size=21,     # ~1 month of testing\n",
        "        target_horizon=5  # Predict 5 days ahead\n",
        "    )\n",
        "\n",
        "    # Print split sizes\n",
        "    X_train_dict, X_val_dict, X_test_dict, y_train_dict, y_val_dict, y_test_dict = splits\n",
        "\n",
        "    for stock in X_train_dict.keys():\n",
        "        print(f\"\\nSplit sizes for {stock}:\")\n",
        "        print(f\"Train: {X_train_dict[stock].shape}\")\n",
        "        print(f\"Val: {X_val_dict[stock].shape}\")\n",
        "        print(f\"Test: {X_test_dict[stock].shape}\")\n",
        "\n",
        "        # Print target statistics\n",
        "        print(f\"Target mean (train): {y_train_dict[stock].mean():.4f}\")\n",
        "        print(f\"Target std (train): {y_train_dict[stock].std():.4f}\")\n",
        "\n",
        "    return processed_df, splits\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    processed_df, splits = test_data_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdukiHMzboS3",
        "outputId": "cb21bbc9-869f-41f7-bc6c-e9bcd64cc7ef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing data pipeline...\n",
            "\n",
            "Loading and processing data...\n",
            "\n",
            "Processed data shape: (13235, 44)\n",
            "\n",
            "Feature columns:\n",
            "['Dates', 'Stock', 'Last Price', 'Open Price', 'High Price', 'Low Price', 'Volume', 'Bid Price', 'Ask Price', 'Market Order Bid Size', 'Market Order Ask Size', 'Average Volume 30 Day', 'Moving Average 20 Day', 'Turnover / Traded Value', 'RSI 14 Day', 'Moving Average 50 Day', 'Moving Average 200 Day', 'Volatility 30 Day', 'Implied Volatility Mid', 'Current Market Cap', 'VWAP Price', 'ROC_5', 'ROC_10', 'ROC_20', 'MACD', 'MACD_Signal', 'MACD_Diff', 'BB_High', 'BB_Mid', 'BB_Low', 'BB_Width', 'ATR', 'OBV', 'ADI', 'Daily_Return', 'Log_Return', 'High_Low_Range', 'Price_MA_Ratio_20', 'Price_MA_Ratio_50', 'Price_MA_Ratio_200', 'Distance_To_MA_20', 'Distance_To_MA_50', 'Volume_MA_Ratio', 'Volume_Price_Trend']\n",
            "\n",
            "Creating train/val/test splits...\n",
            "\n",
            "Split info for ALT US Equity:\n",
            "Training period: 2023-11-10 00:00:00 to 2024-10-28 00:00:00\n",
            "Validation period: 2024-10-29 00:00:00 to 2025-01-23 00:00:00\n",
            "Test period: 2025-01-24 00:00:00 to 2025-02-21 00:00:00\n",
            "Training samples: 252\n",
            "Validation samples: 63\n",
            "Test samples: 21\n",
            "\n",
            "Split info for CELH US Equity:\n",
            "Training period: 2023-11-10 00:00:00 to 2024-10-28 00:00:00\n",
            "Validation period: 2024-10-29 00:00:00 to 2025-01-23 00:00:00\n",
            "Test period: 2025-01-24 00:00:00 to 2025-02-21 00:00:00\n",
            "Training samples: 252\n",
            "Validation samples: 63\n",
            "Test samples: 21\n",
            "\n",
            "Split info for CVNA US Equity:\n",
            "Training period: 2023-11-10 00:00:00 to 2024-10-28 00:00:00\n",
            "Validation period: 2024-10-29 00:00:00 to 2025-01-23 00:00:00\n",
            "Test period: 2025-01-24 00:00:00 to 2025-02-21 00:00:00\n",
            "Training samples: 252\n",
            "Validation samples: 63\n",
            "Test samples: 21\n",
            "\n",
            "Split info for FUBO US Equity:\n",
            "Training period: 2023-11-10 00:00:00 to 2024-10-28 00:00:00\n",
            "Validation period: 2024-10-29 00:00:00 to 2025-01-23 00:00:00\n",
            "Test period: 2025-01-24 00:00:00 to 2025-02-21 00:00:00\n",
            "Training samples: 252\n",
            "Validation samples: 63\n",
            "Test samples: 21\n",
            "\n",
            "Split info for UPST US Equity:\n",
            "Training period: 2023-11-10 00:00:00 to 2024-10-28 00:00:00\n",
            "Validation period: 2024-10-29 00:00:00 to 2025-01-23 00:00:00\n",
            "Test period: 2025-01-24 00:00:00 to 2025-02-21 00:00:00\n",
            "Training samples: 252\n",
            "Validation samples: 63\n",
            "Test samples: 21\n",
            "\n",
            "Split sizes for ALT US Equity:\n",
            "Train: (252, 34)\n",
            "Val: (63, 34)\n",
            "Test: (21, 34)\n",
            "Target mean (train): 0.0343\n",
            "Target std (train): 0.1850\n",
            "\n",
            "Split sizes for CELH US Equity:\n",
            "Train: (252, 34)\n",
            "Val: (63, 34)\n",
            "Test: (21, 34)\n",
            "Target mean (train): -0.0067\n",
            "Target std (train): 0.0903\n",
            "\n",
            "Split sizes for CVNA US Equity:\n",
            "Train: (252, 34)\n",
            "Val: (63, 34)\n",
            "Test: (21, 34)\n",
            "Target mean (train): 0.0468\n",
            "Target std (train): 0.1279\n",
            "\n",
            "Split sizes for FUBO US Equity:\n",
            "Train: (252, 34)\n",
            "Val: (63, 34)\n",
            "Test: (21, 34)\n",
            "Target mean (train): -0.0046\n",
            "Target std (train): 0.1115\n",
            "\n",
            "Split sizes for UPST US Equity:\n",
            "Train: (252, 34)\n",
            "Val: (63, 34)\n",
            "Test: (21, 34)\n",
            "Target mean (train): 0.0224\n",
            "Target std (train): 0.1296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import logging\n",
        "import time\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Check for CUDA\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logger.info(f\"Using device: {DEVICE}\")\n",
        "\n",
        "class StockDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for stock data\"\"\"\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        self.X = torch.FloatTensor(X)\n",
        "        self.y = torch.FloatTensor(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class StockNN(nn.Module):\n",
        "    \"\"\"Neural Network for stock prediction\"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dims: List[int] = [128, 64, 32]):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        # Build hidden layers\n",
        "        for dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, dim),\n",
        "                nn.BatchNorm1d(dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.3)\n",
        "            ])\n",
        "            prev_dim = dim\n",
        "\n",
        "        # Output layer\n",
        "        layers.append(nn.Linear(prev_dim, 1))\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x).squeeze()\n",
        "\n",
        "class ModelEnsemble:\n",
        "    \"\"\"Ensemble of models for stock prediction\"\"\"\n",
        "    def __init__(self, input_dim: int, model_params: Dict = None):\n",
        "        self.input_dim = input_dim\n",
        "        self.model_params = model_params or self._default_params()\n",
        "\n",
        "        # Initialize models with appropriate device settings\n",
        "        use_gpu = torch.cuda.is_available()\n",
        "\n",
        "        # Initialize CatBoost\n",
        "        catboost_params = self.model_params['catboost'].copy()\n",
        "        if use_gpu:\n",
        "            catboost_params['task_type'] = 'GPU'\n",
        "            logger.info(\"CatBoost will use GPU\")\n",
        "\n",
        "        # Initialize LightGBM (using CPU as it's more stable)\n",
        "        lightgbm_params = self.model_params['lightgbm'].copy()\n",
        "        logger.info(\"LightGBM will use CPU for stability\")\n",
        "\n",
        "        self.models = {\n",
        "            'catboost': CatBoostRegressor(**catboost_params),\n",
        "            'lightgbm': LGBMRegressor(**lightgbm_params),\n",
        "            'nn': StockNN(input_dim, self.model_params['nn']['hidden_dims']).to(DEVICE)\n",
        "        }\n",
        "\n",
        "        self.model_weights = None\n",
        "\n",
        "    def _default_params(self) -> Dict:\n",
        "\n",
        "      return {\n",
        "          'catboost': {\n",
        "              'iterations': 500,           # Reduced from 1000\n",
        "              'learning_rate': 0.02,       # Reduced from 0.03\n",
        "              'depth': 4,                  # Reduced from 6\n",
        "              'l2_leaf_reg': 5,           # Increased from 3\n",
        "              'verbose': False\n",
        "          },\n",
        "          'lightgbm': {\n",
        "              'n_estimators': 500,        # Reduced from 1000\n",
        "              'learning_rate': 0.02,      # Reduced from 0.03\n",
        "              'num_leaves': 16,           # Reduced from 32\n",
        "              'feature_fraction': 0.7,    # Reduced from 0.8\n",
        "              'subsample': 0.7,           # Reduced from 0.8\n",
        "              'verbose': -1\n",
        "          },\n",
        "          'nn': {\n",
        "              'hidden_dims': [64, 32],    # Simplified architecture\n",
        "              'lr': 0.0005,               # Reduced from 0.001\n",
        "              'batch_size': 32,           # Reduced from 64\n",
        "              'epochs': 50                # Reduced from 100\n",
        "          }\n",
        "      }\n",
        "\n",
        "    def train_nn(self, X: np.ndarray, y: np.ndarray, X_val: np.ndarray, y_val: np.ndarray):\n",
        "        \"\"\"Train neural network\"\"\"\n",
        "        # Create datasets\n",
        "        train_dataset = StockDataset(X, y)\n",
        "        val_dataset = StockDataset(X_val, y_val)\n",
        "\n",
        "        # Create dataloaders\n",
        "        train_loader = DataLoader(train_dataset,\n",
        "                                batch_size=self.model_params['nn']['batch_size'],\n",
        "                                shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset,\n",
        "                              batch_size=self.model_params['nn']['batch_size'])\n",
        "\n",
        "        # Initialize optimizer and loss\n",
        "        optimizer = torch.optim.Adam(self.models['nn'].parameters(),\n",
        "                                   lr=self.model_params['nn']['lr'])\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        # Training loop\n",
        "        best_val_loss = float('inf')\n",
        "        patience = 10\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(self.model_params['nn']['epochs']):\n",
        "            # Training\n",
        "            self.models['nn'].train()\n",
        "            train_loss = 0\n",
        "            for batch_X, batch_y in train_loader:\n",
        "                batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = self.models['nn'](batch_X)\n",
        "                loss = criterion(y_pred, batch_y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "\n",
        "            # Validation\n",
        "            self.models['nn'].eval()\n",
        "            val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for batch_X, batch_y in val_loader:\n",
        "                    batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n",
        "                    y_pred = self.models['nn'](batch_X)\n",
        "                    val_loss += criterion(y_pred, batch_y).item()\n",
        "\n",
        "            # Early stopping\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= patience:\n",
        "                logger.info(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                logger.info(f\"Epoch {epoch}: Train Loss = {train_loss/len(train_loader):.4f}, \"\n",
        "                          f\"Val Loss = {val_loss/len(val_loader):.4f}\")\n",
        "\n",
        "    def train(self, X: np.ndarray, y: np.ndarray, X_val: np.ndarray, y_val: np.ndarray):\n",
        "        \"\"\"Train all models\"\"\"\n",
        "        logger.info(\"Starting model training...\")\n",
        "\n",
        "        # Train CatBoost\n",
        "        logger.info(\"\\nTraining CatBoost...\")\n",
        "        start_time = time.time()\n",
        "        self.models['catboost'].fit(X, y, eval_set=(X_val, y_val))\n",
        "        catboost_time = time.time() - start_time\n",
        "        logger.info(f\"CatBoost training completed in {catboost_time:.2f} seconds\")\n",
        "\n",
        "        # Train LightGBM\n",
        "        logger.info(\"\\nTraining LightGBM...\")\n",
        "        start_time = time.time()\n",
        "        self.models['lightgbm'].fit(X, y, eval_set=(X_val, y_val))\n",
        "        lightgbm_time = time.time() - start_time\n",
        "        logger.info(f\"LightGBM training completed in {lightgbm_time:.2f} seconds\")\n",
        "\n",
        "        # Train Neural Network\n",
        "        logger.info(\"\\nTraining Neural Network...\")\n",
        "        start_time = time.time()\n",
        "        self.train_nn(X, y, X_val, y_val)\n",
        "        nn_time = time.time() - start_time\n",
        "        logger.info(f\"Neural Network training completed in {nn_time:.2f} seconds\")\n",
        "\n",
        "        # Calculate model weights based on validation performance\n",
        "        predictions = {\n",
        "            name: model.predict(X_val) if name != 'nn' else\n",
        "            model(torch.FloatTensor(X_val).to(DEVICE)).cpu().detach().numpy()\n",
        "            for name, model in self.models.items()\n",
        "        }\n",
        "\n",
        "        errors = {\n",
        "            name: mean_squared_error(y_val, pred)\n",
        "            for name, pred in predictions.items()\n",
        "        }\n",
        "\n",
        "        # Convert errors to weights (lower error = higher weight)\n",
        "        total_error = sum(1/e for e in errors.values())\n",
        "        self.model_weights = {\n",
        "            name: (1/error)/total_error\n",
        "            for name, error in errors.items()\n",
        "        }\n",
        "\n",
        "        logger.info(\"Model weights:\")\n",
        "        for name, weight in self.model_weights.items():\n",
        "            logger.info(f\"{name}: {weight:.4f}\")\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Make predictions using ensemble\"\"\"\n",
        "        predictions = {\n",
        "            name: model.predict(X) if name != 'nn' else\n",
        "            model(torch.FloatTensor(X).to(DEVICE)).cpu().detach().numpy()\n",
        "            for name, model in self.models.items()\n",
        "        }\n",
        "\n",
        "        # Weighted average of predictions\n",
        "        ensemble_pred = sum(pred * self.model_weights[name]\n",
        "                          for name, pred in predictions.items())\n",
        "\n",
        "        return ensemble_pred\n",
        "\n",
        "    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"Evaluate model performance\"\"\"\n",
        "        y_pred = self.predict(X)\n",
        "\n",
        "        return {\n",
        "            'mae': mean_absolute_error(y, y_pred),\n",
        "            'rmse': np.sqrt(mean_squared_error(y, y_pred)),\n",
        "            'r2': r2_score(y, y_pred)\n",
        "        }\n",
        "\n",
        "def test_model():\n",
        "    \"\"\"Test the model architecture\"\"\"\n",
        "    # Create dummy data\n",
        "    X = np.random.randn(1000, 34)\n",
        "    y = np.random.randn(1000)\n",
        "    X_val = np.random.randn(200, 34)\n",
        "    y_val = np.random.randn(200)\n",
        "    X_test = np.random.randn(100, 34)\n",
        "    y_test = np.random.randn(100)\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = ModelEnsemble(input_dim=34)\n",
        "    model.train(X, y, X_val, y_val)\n",
        "\n",
        "    # Evaluate\n",
        "    metrics = model.evaluate(X_test, y_test)\n",
        "    print(\"\\nTest metrics:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    return model, metrics\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, metrics = test_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzVfoJZQrIme",
        "outputId": "9ff39f81-8059-449a-ff02-d1f6bc2fbfc5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test metrics:\n",
            "mae: 0.7766\n",
            "rmse: 0.9675\n",
            "r2: -0.0784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # This will silence the deprecation warnings\n",
        "\n",
        "# Reset and reconfigure logging\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(message)s',  # Simplified format\n",
        "    force=True\n",
        ")\n",
        "\n",
        "# Initialize preprocessor and load data\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STARTING TRAINING PIPELINE\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "try:\n",
        "    # Initialize preprocessor\n",
        "    preprocessor = StockPreprocessor()\n",
        "\n",
        "    # Load and process data\n",
        "    print(\"1. Loading and processing data...\")\n",
        "    start_time = time.time()\n",
        "    processed_df = preprocessor.load_all_data()\n",
        "    print(f\"   Raw data loaded. Shape: {processed_df.shape}\")\n",
        "\n",
        "    # Add technical features\n",
        "    print(\"\\n2. Adding technical features...\")\n",
        "    processed_df = preprocessor.add_technical_features(processed_df)\n",
        "    print(f\"   Features added. New shape: {processed_df.shape}\")\n",
        "    process_time = time.time() - start_time\n",
        "    print(f\"   Data processing completed in {process_time:.2f} seconds\")\n",
        "\n",
        "    # Create train/val/test splits\n",
        "    print(\"\\n3. Creating data splits...\")\n",
        "    splits = preprocessor.prepare_data_for_modeling(\n",
        "      processed_df,\n",
        "      window_size=252,  # One year\n",
        "      val_size=63,      # Three months\n",
        "      test_size=21      # One month\n",
        "    )\n",
        "    X_train_dict, X_val_dict, X_test_dict, y_train_dict, y_val_dict, y_test_dict = splits\n",
        "\n",
        "    # Train models for each stock\n",
        "    stock_models = {}\n",
        "    print(\"\\n4. Training models for each stock...\")\n",
        "    for stock in X_train_dict.keys():\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Training models for {stock}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Get data for this stock\n",
        "        X_train = X_train_dict[stock]\n",
        "        X_val = X_val_dict[stock]\n",
        "        X_test = X_test_dict[stock]\n",
        "        y_train = y_train_dict[stock]\n",
        "        y_val = y_val_dict[stock]\n",
        "        y_test = y_test_dict[stock]\n",
        "\n",
        "        print(f\"\\nData shapes:\")\n",
        "        print(f\"Train: {X_train.shape}\")\n",
        "        print(f\"Validation: {X_val.shape}\")\n",
        "        print(f\"Test: {X_test.shape}\")\n",
        "\n",
        "        # Train models\n",
        "        start_time = time.time()\n",
        "        model = ModelEnsemble(input_dim=X_train.shape[1])\n",
        "        model.train(X_train, y_train, X_val, y_val)\n",
        "\n",
        "        # Evaluate\n",
        "        train_metrics = model.evaluate(X_train, y_train)\n",
        "        val_metrics = model.evaluate(X_val, y_val)\n",
        "        test_metrics = model.evaluate(X_test, y_test)\n",
        "\n",
        "        print(\"\\nModel Performance:\")\n",
        "        print(\"\\nTrain metrics:\")\n",
        "        for metric, value in train_metrics.items():\n",
        "            print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "        print(\"\\nValidation metrics:\")\n",
        "        for metric, value in val_metrics.items():\n",
        "            print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "        print(\"\\nTest metrics:\")\n",
        "        for metric, value in test_metrics.items():\n",
        "            print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "        stock_models[stock] = model\n",
        "\n",
        "    # Print final summary\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FINAL PERFORMANCE SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for stock in stock_models:\n",
        "        print(f\"\\n{stock}:\")\n",
        "        metrics = stock_models[stock].evaluate(X_test_dict[stock], y_test_dict[stock])\n",
        "        for metric, value in metrics.items():\n",
        "            print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nError during training: {str(e)}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hvEPeCKs5u6",
        "outputId": "b6bf0636-fdc7-46b8-ff99-58af6595781c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "STARTING TRAINING PIPELINE\n",
            "==================================================\n",
            "\n",
            "1. Loading and processing data...\n",
            "   Raw data loaded. Shape: (13235, 22)\n",
            "\n",
            "2. Adding technical features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Features added. New shape: (13235, 44)\n",
            "   Data processing completed in 0.32 seconds\n",
            "\n",
            "3. Creating data splits...\n",
            "\n",
            "Split info for ALT US Equity:\n",
            "Training period: 2023-11-10 00:00:00 to 2024-10-28 00:00:00\n",
            "Validation period: 2024-10-29 00:00:00 to 2025-01-23 00:00:00\n",
            "Test period: 2025-01-24 00:00:00 to 2025-02-21 00:00:00\n",
            "Training samples: 252\n",
            "Validation samples: 63\n",
            "Test samples: 21\n",
            "\n",
            "Split info for CELH US Equity:\n",
            "Training period: 2023-11-10 00:00:00 to 2024-10-28 00:00:00\n",
            "Validation period: 2024-10-29 00:00:00 to 2025-01-23 00:00:00\n",
            "Test period: 2025-01-24 00:00:00 to 2025-02-21 00:00:00\n",
            "Training samples: 252\n",
            "Validation samples: 63\n",
            "Test samples: 21\n",
            "\n",
            "Split info for CVNA US Equity:\n",
            "Training period: 2023-11-10 00:00:00 to 2024-10-28 00:00:00\n",
            "Validation period: 2024-10-29 00:00:00 to 2025-01-23 00:00:00\n",
            "Test period: 2025-01-24 00:00:00 to 2025-02-21 00:00:00\n",
            "Training samples: 252\n",
            "Validation samples: 63\n",
            "Test samples: 21\n",
            "\n",
            "Split info for FUBO US Equity:\n",
            "Training period: 2023-11-10 00:00:00 to 2024-10-28 00:00:00\n",
            "Validation period: 2024-10-29 00:00:00 to 2025-01-23 00:00:00\n",
            "Test period: 2025-01-24 00:00:00 to 2025-02-21 00:00:00\n",
            "Training samples: 252\n",
            "Validation samples: 63\n",
            "Test samples: 21\n",
            "\n",
            "Split info for UPST US Equity:\n",
            "Training period: 2023-11-10 00:00:00 to 2024-10-28 00:00:00\n",
            "Validation period: 2024-10-29 00:00:00 to 2025-01-23 00:00:00\n",
            "Test period: 2025-01-24 00:00:00 to 2025-02-21 00:00:00\n",
            "Training samples: 252\n",
            "Validation samples: 63\n",
            "Test samples: 21\n",
            "\n",
            "4. Training models for each stock...\n",
            "\n",
            "==================================================\n",
            "Training models for ALT US Equity\n",
            "==================================================\n",
            "\n",
            "Data shapes:\n",
            "Train: (252, 34)\n",
            "Validation: (63, 34)\n",
            "Test: (21, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 3.00 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.11 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1939, Val Loss = 0.0237\n",
            "Epoch 10: Train Loss = 0.0922, Val Loss = 0.0222\n",
            "Early stopping at epoch 11\n",
            "Neural Network training completed in 0.25 seconds\n",
            "Model weights:\n",
            "catboost: 0.4870\n",
            "lightgbm: 0.2964\n",
            "nn: 0.2166\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance:\n",
            "\n",
            "Train metrics:\n",
            "mae: 0.0691\n",
            "rmse: 0.1053\n",
            "r2: 0.6756\n",
            "\n",
            "Validation metrics:\n",
            "mae: 0.0786\n",
            "rmse: 0.1052\n",
            "r2: -0.0884\n",
            "\n",
            "Test metrics:\n",
            "mae: 0.0644\n",
            "rmse: 0.0744\n",
            "r2: -1.4080\n",
            "\n",
            "==================================================\n",
            "Training models for CELH US Equity\n",
            "==================================================\n",
            "\n",
            "Data shapes:\n",
            "Train: (252, 34)\n",
            "Validation: (63, 34)\n",
            "Test: (21, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 3.60 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.11 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1494, Val Loss = 0.0065\n",
            "Epoch 10: Train Loss = 0.0569, Val Loss = 0.0101\n",
            "Epoch 20: Train Loss = 0.0301, Val Loss = 0.0089\n",
            "Early stopping at epoch 24\n",
            "Neural Network training completed in 0.52 seconds\n",
            "Model weights:\n",
            "catboost: 0.3652\n",
            "lightgbm: 0.3199\n",
            "nn: 0.3149\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance:\n",
            "\n",
            "Train metrics:\n",
            "mae: 0.0404\n",
            "rmse: 0.0522\n",
            "r2: 0.6660\n",
            "\n",
            "Validation metrics:\n",
            "mae: 0.0615\n",
            "rmse: 0.0729\n",
            "r2: 0.0385\n",
            "\n",
            "Test metrics:\n",
            "mae: 0.1128\n",
            "rmse: 0.1377\n",
            "r2: -0.1401\n",
            "\n",
            "==================================================\n",
            "Training models for CVNA US Equity\n",
            "==================================================\n",
            "\n",
            "Data shapes:\n",
            "Train: (252, 34)\n",
            "Validation: (63, 34)\n",
            "Test: (21, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 3.39 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.10 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1742, Val Loss = 0.0130\n",
            "Early stopping at epoch 10\n",
            "Neural Network training completed in 0.21 seconds\n",
            "Model weights:\n",
            "catboost: 0.6291\n",
            "lightgbm: 0.2702\n",
            "nn: 0.1007\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance:\n",
            "\n",
            "Train metrics:\n",
            "mae: 0.0578\n",
            "rmse: 0.0793\n",
            "r2: 0.6159\n",
            "\n",
            "Validation metrics:\n",
            "mae: 0.0774\n",
            "rmse: 0.0998\n",
            "r2: -0.5789\n",
            "\n",
            "Test metrics:\n",
            "mae: 0.0578\n",
            "rmse: 0.0836\n",
            "r2: -0.6363\n",
            "\n",
            "==================================================\n",
            "Training models for FUBO US Equity\n",
            "==================================================\n",
            "\n",
            "Data shapes:\n",
            "Train: (252, 34)\n",
            "Validation: (63, 34)\n",
            "Test: (21, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 3.71 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.10 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.2444, Val Loss = 1.8285\n",
            "Early stopping at epoch 10\n",
            "Neural Network training completed in 0.22 seconds\n",
            "Model weights:\n",
            "catboost: 0.4931\n",
            "lightgbm: 0.4858\n",
            "nn: 0.0211\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance:\n",
            "\n",
            "Train metrics:\n",
            "mae: 0.0214\n",
            "rmse: 0.0287\n",
            "r2: 0.9338\n",
            "\n",
            "Validation metrics:\n",
            "mae: 0.3310\n",
            "rmse: 0.8301\n",
            "r2: 0.0004\n",
            "\n",
            "Test metrics:\n",
            "mae: 0.0709\n",
            "rmse: 0.0920\n",
            "r2: -0.2071\n",
            "\n",
            "==================================================\n",
            "Training models for UPST US Equity\n",
            "==================================================\n",
            "\n",
            "Data shapes:\n",
            "Train: (252, 34)\n",
            "Validation: (63, 34)\n",
            "Test: (21, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 3.65 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.11 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.2083, Val Loss = 0.1125\n",
            "Epoch 10: Train Loss = 0.0932, Val Loss = 0.0593\n",
            "Epoch 20: Train Loss = 0.0508, Val Loss = 0.0781\n",
            "Early stopping at epoch 23\n",
            "Neural Network training completed in 0.48 seconds\n",
            "Model weights:\n",
            "catboost: 0.4422\n",
            "lightgbm: 0.4234\n",
            "nn: 0.1344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance:\n",
            "\n",
            "Train metrics:\n",
            "mae: 0.0479\n",
            "rmse: 0.0659\n",
            "r2: 0.7417\n",
            "\n",
            "Validation metrics:\n",
            "mae: 0.1159\n",
            "rmse: 0.1616\n",
            "r2: -0.0702\n",
            "\n",
            "Test metrics:\n",
            "mae: 0.0968\n",
            "rmse: 0.1218\n",
            "r2: 0.1349\n",
            "\n",
            "==================================================\n",
            "FINAL PERFORMANCE SUMMARY\n",
            "==================================================\n",
            "\n",
            "ALT US Equity:\n",
            "mae: 0.0644\n",
            "rmse: 0.0744\n",
            "r2: -1.4080\n",
            "\n",
            "CELH US Equity:\n",
            "mae: 0.1128\n",
            "rmse: 0.1377\n",
            "r2: -0.1401\n",
            "\n",
            "CVNA US Equity:\n",
            "mae: 0.0578\n",
            "rmse: 0.0836\n",
            "r2: -0.6363\n",
            "\n",
            "FUBO US Equity:\n",
            "mae: 0.0709\n",
            "rmse: 0.0920\n",
            "r2: -0.2071\n",
            "\n",
            "UPST US Equity:\n",
            "mae: 0.0968\n",
            "rmse: 0.1218\n",
            "r2: 0.1349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNYt2crc4x72",
        "outputId": "d7a79987-b9b5-474a-fa9d-4770cd707d87"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import optuna\n",
        "import numpy as np\n",
        "\n",
        "# Choose one stock for tuning (e.g., 'CELH US Equity')\n",
        "stock = 'CELH US Equity'\n",
        "X_train = X_train_dict[stock]\n",
        "y_train = y_train_dict[stock]\n",
        "X_val = X_val_dict[stock]\n",
        "y_val = y_val_dict[stock]\n",
        "\n",
        "def objective(trial):\n",
        "    # Define hyperparameters for CatBoost\n",
        "    catboost_params = {\n",
        "         'iterations': trial.suggest_int('catboost_iterations', 200, 1000),\n",
        "         'learning_rate': trial.suggest_loguniform('catboost_lr', 0.001, 0.1),\n",
        "         'depth': trial.suggest_int('catboost_depth', 3, 10),\n",
        "         'l2_leaf_reg': trial.suggest_int('catboost_l2_leaf_reg', 1, 10),\n",
        "         'verbose': False,\n",
        "    }\n",
        "\n",
        "    # Define hyperparameters for LightGBM\n",
        "    lightgbm_params = {\n",
        "         'n_estimators': trial.suggest_int('lgbm_n_estimators', 200, 1000),\n",
        "         'learning_rate': trial.suggest_loguniform('lgbm_lr', 0.001, 0.1),\n",
        "         'num_leaves': trial.suggest_int('lgbm_num_leaves', 8, 64),\n",
        "         'feature_fraction': trial.suggest_uniform('lgbm_feature_fraction', 0.5, 1.0),\n",
        "         'subsample': trial.suggest_uniform('lgbm_subsample', 0.5, 1.0),\n",
        "         'verbose': -1,\n",
        "    }\n",
        "\n",
        "    # Define hyperparameters for the Neural Network\n",
        "    nn_hidden_dims = [trial.suggest_int('nn_hidden_dim1', 16, 128),\n",
        "                      trial.suggest_int('nn_hidden_dim2', 16, 128)]\n",
        "    nn_params = {\n",
        "         'hidden_dims': nn_hidden_dims,\n",
        "         'lr': trial.suggest_loguniform('nn_lr', 1e-4, 1e-2),\n",
        "         'batch_size': trial.suggest_categorical('nn_batch_size', [16, 32, 64]),\n",
        "         'epochs': trial.suggest_int('nn_epochs', 10, 50),\n",
        "    }\n",
        "\n",
        "    # Create a combined parameters dictionary for your ModelEnsemble\n",
        "    model_params = {\n",
        "         'catboost': catboost_params,\n",
        "         'lightgbm': lightgbm_params,\n",
        "         'nn': nn_params,\n",
        "    }\n",
        "\n",
        "    # Initialize the model ensemble with the trial hyperparameters\n",
        "    model = ModelEnsemble(input_dim=X_train.shape[1], model_params=model_params)\n",
        "\n",
        "    # Train the ensemble on the training set and evaluate on the validation set\n",
        "    model.train(X_train, y_train, X_val, y_val)\n",
        "    metrics = model.evaluate(X_val, y_val)\n",
        "\n",
        "    # We choose RMSE as our objective (lower is better)\n",
        "    rmse = metrics['rmse']\n",
        "    print(f\"Trial RMSE: {rmse:.4f}\")\n",
        "    return rmse\n",
        "\n",
        "# Create an Optuna study to minimize RMSE\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "# Print the best trial results\n",
        "print(\"\\nBest trial:\")\n",
        "best_trial = study.best_trial\n",
        "print(f\"  RMSE: {best_trial.value:.4f}\")\n",
        "print(\"  Best hyperparameters:\")\n",
        "for key, value in best_trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnkodHtCx2JW",
        "outputId": "b7efd08f-08b0-459a-e47d-7a47c830dff1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 12:21:10,351] A new study created in memory with name: no-name-2d349bc6-0c16-4228-8837-f31afcd7303b\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n",
            "CatBoost training completed in 1.32 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.21 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1377, Val Loss = 0.0074\n",
            "Epoch 10: Train Loss = 0.0249, Val Loss = 0.0058\n",
            "Epoch 20: Train Loss = 0.0104, Val Loss = 0.0070\n",
            "Early stopping at epoch 25\n",
            "Neural Network training completed in 0.98 seconds\n",
            "Model weights:\n",
            "catboost: 0.3566\n",
            "lightgbm: 0.3029\n",
            "nn: 0.3405\n",
            "[I 2025-02-24 12:21:12,882] Trial 0 finished with value: 0.07141411322068592 and parameters: {'catboost_iterations': 410, 'catboost_lr': 0.0030260977729874524, 'catboost_depth': 7, 'catboost_l2_leaf_reg': 5, 'lgbm_n_estimators': 898, 'lgbm_lr': 0.030082522492109017, 'lgbm_num_leaves': 22, 'lgbm_feature_fraction': 0.8553354378822933, 'lgbm_subsample': 0.6369350411573018, 'nn_hidden_dim1': 84, 'nn_hidden_dim2': 84, 'nn_lr': 0.0011283754000402794, 'nn_batch_size': 16, 'nn_epochs': 42}. Best is trial 0 with value: 0.07141411322068592.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0714\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 1.83 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.19 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1456, Val Loss = 0.0157\n",
            "Epoch 10: Train Loss = 0.0532, Val Loss = 0.0072\n",
            "Epoch 20: Train Loss = 0.0261, Val Loss = 0.0068\n",
            "Neural Network training completed in 0.40 seconds\n",
            "Model weights:\n",
            "catboost: 0.3696\n",
            "lightgbm: 0.3632\n",
            "nn: 0.2672\n",
            "[I 2025-02-24 12:21:15,322] Trial 1 finished with value: 0.07125512014290697 and parameters: {'catboost_iterations': 410, 'catboost_lr': 0.016928803942401944, 'catboost_depth': 9, 'catboost_l2_leaf_reg': 8, 'lgbm_n_estimators': 835, 'lgbm_lr': 0.005459269665792007, 'lgbm_num_leaves': 32, 'lgbm_feature_fraction': 0.9246332368772088, 'lgbm_subsample': 0.9112110780121226, 'nn_hidden_dim1': 125, 'nn_hidden_dim2': 79, 'nn_lr': 0.0006509094202222716, 'nn_batch_size': 32, 'nn_epochs': 21}. Best is trial 1 with value: 0.07125512014290697.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 1.47 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.07 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.2078, Val Loss = 0.0092\n",
            "Epoch 10: Train Loss = 0.0094, Val Loss = 0.0045\n",
            "Neural Network training completed in 0.18 seconds\n",
            "Model weights:\n",
            "catboost: 0.3526\n",
            "lightgbm: 0.2904\n",
            "nn: 0.3569\n",
            "[I 2025-02-24 12:21:17,055] Trial 2 finished with value: 0.06864626554114395 and parameters: {'catboost_iterations': 397, 'catboost_lr': 0.07258515763234784, 'catboost_depth': 6, 'catboost_l2_leaf_reg': 9, 'lgbm_n_estimators': 279, 'lgbm_lr': 0.03730170563724479, 'lgbm_num_leaves': 59, 'lgbm_feature_fraction': 0.9888008758487448, 'lgbm_subsample': 0.7084457404236191, 'nn_hidden_dim1': 45, 'nn_hidden_dim2': 60, 'nn_lr': 0.007818319401910925, 'nn_batch_size': 64, 'nn_epochs': 16}. Best is trial 2 with value: 0.06864626554114395.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 4.89 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.09 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.2327, Val Loss = 0.0239\n",
            "Epoch 10: Train Loss = 0.0365, Val Loss = 0.0071\n",
            "Epoch 20: Train Loss = 0.0175, Val Loss = 0.0047\n",
            "Early stopping at epoch 28\n",
            "Neural Network training completed in 1.04 seconds\n",
            "Model weights:\n",
            "catboost: 0.3051\n",
            "lightgbm: 0.3123\n",
            "nn: 0.3826\n",
            "[I 2025-02-24 12:21:23,094] Trial 3 finished with value: 0.06871425388865587 and parameters: {'catboost_iterations': 787, 'catboost_lr': 0.09746532311717716, 'catboost_depth': 3, 'catboost_l2_leaf_reg': 9, 'lgbm_n_estimators': 448, 'lgbm_lr': 0.005396868144334628, 'lgbm_num_leaves': 49, 'lgbm_feature_fraction': 0.6085698379264205, 'lgbm_subsample': 0.5991268368134505, 'nn_hidden_dim1': 103, 'nn_hidden_dim2': 103, 'nn_lr': 0.0008898826157972938, 'nn_batch_size': 16, 'nn_epochs': 33}. Best is trial 2 with value: 0.06864626554114395.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 0.99 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.10 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1156, Val Loss = 0.0102\n",
            "Epoch 10: Train Loss = 0.0081, Val Loss = 0.0057\n",
            "Early stopping at epoch 14\n",
            "Neural Network training completed in 0.29 seconds\n",
            "Model weights:\n",
            "catboost: 0.3243\n",
            "lightgbm: 0.3562\n",
            "nn: 0.3196\n",
            "[I 2025-02-24 12:21:24,502] Trial 4 finished with value: 0.07240826929757076 and parameters: {'catboost_iterations': 214, 'catboost_lr': 0.0012176489421967217, 'catboost_depth': 9, 'catboost_l2_leaf_reg': 6, 'lgbm_n_estimators': 457, 'lgbm_lr': 0.0010101866913055574, 'lgbm_num_leaves': 45, 'lgbm_feature_fraction': 0.9848580777459912, 'lgbm_subsample': 0.6717103064374932, 'nn_hidden_dim1': 23, 'nn_hidden_dim2': 24, 'nn_lr': 0.005666686713391713, 'nn_batch_size': 32, 'nn_epochs': 46}. Best is trial 2 with value: 0.06864626554114395.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 1.84 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.06 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.3013, Val Loss = 0.0943\n",
            "Epoch 10: Train Loss = 0.1632, Val Loss = 0.0463\n",
            "Epoch 20: Train Loss = 0.1297, Val Loss = 0.0204\n",
            "Neural Network training completed in 0.30 seconds\n",
            "Model weights:\n",
            "catboost: 0.4333\n",
            "lightgbm: 0.4669\n",
            "nn: 0.0998\n",
            "[I 2025-02-24 12:21:26,727] Trial 5 finished with value: 0.07172804628918408 and parameters: {'catboost_iterations': 445, 'catboost_lr': 0.009533839413165332, 'catboost_depth': 6, 'catboost_l2_leaf_reg': 8, 'lgbm_n_estimators': 279, 'lgbm_lr': 0.0033349487609136553, 'lgbm_num_leaves': 20, 'lgbm_feature_fraction': 0.8715673739189072, 'lgbm_subsample': 0.7836850006759823, 'nn_hidden_dim1': 58, 'nn_hidden_dim2': 112, 'nn_lr': 0.0001004670680537258, 'nn_batch_size': 64, 'nn_epochs': 27}. Best is trial 2 with value: 0.06864626554114395.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 25.02 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.10 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1950, Val Loss = 0.0078\n",
            "Epoch 10: Train Loss = 0.0151, Val Loss = 0.0082\n",
            "Epoch 20: Train Loss = 0.0070, Val Loss = 0.0049\n",
            "Epoch 30: Train Loss = 0.0067, Val Loss = 0.0045\n",
            "Epoch 40: Train Loss = 0.0060, Val Loss = 0.0049\n",
            "Early stopping at epoch 43\n",
            "Neural Network training completed in 0.85 seconds\n",
            "Model weights:\n",
            "catboost: 0.3619\n",
            "lightgbm: 0.3526\n",
            "nn: 0.2855\n",
            "[I 2025-02-24 12:21:52,719] Trial 6 finished with value: 0.06963497862811235 and parameters: {'catboost_iterations': 835, 'catboost_lr': 0.09534868500565148, 'catboost_depth': 9, 'catboost_l2_leaf_reg': 2, 'lgbm_n_estimators': 486, 'lgbm_lr': 0.0016661602171931119, 'lgbm_num_leaves': 52, 'lgbm_feature_fraction': 0.9369047650742622, 'lgbm_subsample': 0.7744916357861032, 'nn_hidden_dim1': 51, 'nn_hidden_dim2': 73, 'nn_lr': 0.003158232175066944, 'nn_batch_size': 32, 'nn_epochs': 49}. Best is trial 2 with value: 0.06864626554114395.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 5.16 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.07 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.0991, Val Loss = 0.0176\n",
            "Epoch 10: Train Loss = 0.0101, Val Loss = 0.0118\n",
            "Early stopping at epoch 15\n",
            "Neural Network training completed in 0.59 seconds\n",
            "Model weights:\n",
            "catboost: 0.3707\n",
            "lightgbm: 0.3561\n",
            "nn: 0.2732\n",
            "[I 2025-02-24 12:21:58,561] Trial 7 finished with value: 0.07395038712471887 and parameters: {'catboost_iterations': 861, 'catboost_lr': 0.010587298989949968, 'catboost_depth': 4, 'catboost_l2_leaf_reg': 5, 'lgbm_n_estimators': 336, 'lgbm_lr': 0.04726904229128165, 'lgbm_num_leaves': 27, 'lgbm_feature_fraction': 0.7208260838623748, 'lgbm_subsample': 0.7437181227143066, 'nn_hidden_dim1': 22, 'nn_hidden_dim2': 94, 'nn_lr': 0.00906129475342404, 'nn_batch_size': 16, 'nn_epochs': 18}. Best is trial 2 with value: 0.06864626554114395.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0740\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 0.92 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.14 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.0806, Val Loss = 0.0188\n",
            "Epoch 10: Train Loss = 0.0083, Val Loss = 0.0053\n",
            "Early stopping at epoch 12\n",
            "Neural Network training completed in 0.47 seconds\n",
            "Model weights:\n",
            "catboost: 0.3223\n",
            "lightgbm: 0.3434\n",
            "nn: 0.3343\n",
            "[I 2025-02-24 12:22:00,110] Trial 8 finished with value: 0.0697805817820682 and parameters: {'catboost_iterations': 253, 'catboost_lr': 0.030211478484398775, 'catboost_depth': 3, 'catboost_l2_leaf_reg': 1, 'lgbm_n_estimators': 675, 'lgbm_lr': 0.002637619195161469, 'lgbm_num_leaves': 23, 'lgbm_feature_fraction': 0.9310868052258889, 'lgbm_subsample': 0.8721007653466218, 'nn_hidden_dim1': 38, 'nn_hidden_dim2': 64, 'nn_lr': 0.0060661726663488735, 'nn_batch_size': 16, 'nn_epochs': 24}. Best is trial 2 with value: 0.06864626554114395.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 4.78 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.11 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.2014, Val Loss = 0.0158\n",
            "Epoch 10: Train Loss = 0.1045, Val Loss = 0.0319\n",
            "Early stopping at epoch 11\n",
            "Neural Network training completed in 0.14 seconds\n",
            "Model weights:\n",
            "catboost: 0.4429\n",
            "lightgbm: 0.4756\n",
            "nn: 0.0815\n",
            "[I 2025-02-24 12:22:05,162] Trial 9 finished with value: 0.07040185603728362 and parameters: {'catboost_iterations': 639, 'catboost_lr': 0.015112092646551126, 'catboost_depth': 5, 'catboost_l2_leaf_reg': 7, 'lgbm_n_estimators': 541, 'lgbm_lr': 0.004057564083474834, 'lgbm_num_leaves': 12, 'lgbm_feature_fraction': 0.8214810797323273, 'lgbm_subsample': 0.8931278864175052, 'nn_hidden_dim1': 127, 'nn_hidden_dim2': 123, 'nn_lr': 0.00023761991667889675, 'nn_batch_size': 64, 'nn_epochs': 47}. Best is trial 2 with value: 0.06864626554114395.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 7.82 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.05 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.2979, Val Loss = 0.0105\n",
            "Neural Network training completed in 0.12 seconds\n",
            "Model weights:\n",
            "catboost: 0.4740\n",
            "lightgbm: 0.4178\n",
            "nn: 0.1082\n",
            "[I 2025-02-24 12:22:13,222] Trial 10 finished with value: 0.07417803801738328 and parameters: {'catboost_iterations': 606, 'catboost_lr': 0.04108580979678186, 'catboost_depth': 7, 'catboost_l2_leaf_reg': 10, 'lgbm_n_estimators': 231, 'lgbm_lr': 0.01968737524187364, 'lgbm_num_leaves': 64, 'lgbm_feature_fraction': 0.5293047241823207, 'lgbm_subsample': 0.5055777850861881, 'nn_hidden_dim1': 78, 'nn_hidden_dim2': 46, 'nn_lr': 0.0022495751777231385, 'nn_batch_size': 64, 'nn_epochs': 10}. Best is trial 2 with value: 0.06864626554114395.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 5.93 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.08 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.2247, Val Loss = 0.0349\n",
            "Epoch 10: Train Loss = 0.0451, Val Loss = 0.0072\n",
            "Epoch 20: Train Loss = 0.0211, Val Loss = 0.0052\n",
            "Epoch 30: Train Loss = 0.0127, Val Loss = 0.0050\n",
            "Neural Network training completed in 1.22 seconds\n",
            "Model weights:\n",
            "catboost: 0.3222\n",
            "lightgbm: 0.3132\n",
            "nn: 0.3645\n",
            "[I 2025-02-24 12:22:20,527] Trial 11 finished with value: 0.06863991215552195 and parameters: {'catboost_iterations': 975, 'catboost_lr': 0.09600040197240402, 'catboost_depth': 3, 'catboost_l2_leaf_reg': 10, 'lgbm_n_estimators': 384, 'lgbm_lr': 0.011089727790929023, 'lgbm_num_leaves': 59, 'lgbm_feature_fraction': 0.6323150018160143, 'lgbm_subsample': 0.5686056525053604, 'nn_hidden_dim1': 100, 'nn_hidden_dim2': 48, 'nn_lr': 0.0006379901469381725, 'nn_batch_size': 16, 'nn_epochs': 34}. Best is trial 11 with value: 0.06863991215552195.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 6.60 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.09 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1997, Val Loss = 0.0066\n",
            "Early stopping at epoch 10\n",
            "Neural Network training completed in 0.12 seconds\n",
            "Model weights:\n",
            "catboost: 0.4549\n",
            "lightgbm: 0.3599\n",
            "nn: 0.1852\n",
            "[I 2025-02-24 12:22:27,408] Trial 12 finished with value: 0.0702675002275118 and parameters: {'catboost_iterations': 993, 'catboost_lr': 0.04647869451517788, 'catboost_depth': 5, 'catboost_l2_leaf_reg': 10, 'lgbm_n_estimators': 367, 'lgbm_lr': 0.07871562293260313, 'lgbm_num_leaves': 64, 'lgbm_feature_fraction': 0.701084076554745, 'lgbm_subsample': 0.5130720830819814, 'nn_hidden_dim1': 104, 'nn_hidden_dim2': 46, 'nn_lr': 0.000462138357795621, 'nn_batch_size': 64, 'nn_epochs': 36}. Best is trial 11 with value: 0.06863991215552195.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 4.99 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.14 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1440, Val Loss = 0.0474\n",
            "Epoch 10: Train Loss = 0.0157, Val Loss = 0.0057\n",
            "Neural Network training completed in 0.44 seconds\n",
            "Model weights:\n",
            "catboost: 0.3888\n",
            "lightgbm: 0.3211\n",
            "nn: 0.2901\n",
            "[I 2025-02-24 12:22:33,052] Trial 13 finished with value: 0.07417913782537405 and parameters: {'catboost_iterations': 503, 'catboost_lr': 0.062035018969442406, 'catboost_depth': 6, 'catboost_l2_leaf_reg': 10, 'lgbm_n_estimators': 668, 'lgbm_lr': 0.013589670903379645, 'lgbm_num_leaves': 56, 'lgbm_feature_fraction': 0.6486692567937008, 'lgbm_subsample': 0.578934259823809, 'nn_hidden_dim1': 97, 'nn_hidden_dim2': 49, 'nn_lr': 0.001853682455944841, 'nn_batch_size': 16, 'nn_epochs': 12}. Best is trial 11 with value: 0.06863991215552195.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 4.47 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.05 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.2294, Val Loss = 0.0135\n",
            "Early stopping at epoch 10\n",
            "Neural Network training completed in 0.13 seconds\n",
            "Model weights:\n",
            "catboost: 0.4621\n",
            "lightgbm: 0.5000\n",
            "nn: 0.0379\n",
            "[I 2025-02-24 12:22:37,789] Trial 14 finished with value: 0.07386244842709067 and parameters: {'catboost_iterations': 704, 'catboost_lr': 0.004791783556111426, 'catboost_depth': 4, 'catboost_l2_leaf_reg': 8, 'lgbm_n_estimators': 212, 'lgbm_lr': 0.009327963549511153, 'lgbm_num_leaves': 40, 'lgbm_feature_fraction': 0.7868110414533033, 'lgbm_subsample': 0.7020140695824725, 'nn_hidden_dim1': 59, 'nn_hidden_dim2': 24, 'nn_lr': 0.0002608337317620831, 'nn_batch_size': 64, 'nn_epochs': 38}. Best is trial 11 with value: 0.06863991215552195.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 17.71 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.07 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.3898, Val Loss = 0.0312\n",
            "Epoch 10: Train Loss = 0.0722, Val Loss = 0.0189\n",
            "Epoch 20: Train Loss = 0.0473, Val Loss = 0.0171\n",
            "Neural Network training completed in 1.09 seconds\n",
            "Model weights:\n",
            "catboost: 0.4477\n",
            "lightgbm: 0.3510\n",
            "nn: 0.2013\n",
            "[I 2025-02-24 12:22:56,739] Trial 15 finished with value: 0.0759611627439094 and parameters: {'catboost_iterations': 970, 'catboost_lr': 0.026716907010203583, 'catboost_depth': 8, 'catboost_l2_leaf_reg': 4, 'lgbm_n_estimators': 367, 'lgbm_lr': 0.03563916816426562, 'lgbm_num_leaves': 57, 'lgbm_feature_fraction': 0.529322799999427, 'lgbm_subsample': 0.8287518894964, 'nn_hidden_dim1': 40, 'nn_hidden_dim2': 65, 'nn_lr': 0.00035909579669653283, 'nn_batch_size': 16, 'nn_epochs': 29}. Best is trial 11 with value: 0.06863991215552195.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0760\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 1.99 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.12 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.2773, Val Loss = 0.0065\n",
            "Epoch 10: Train Loss = 0.0604, Val Loss = 0.0535\n",
            "Early stopping at epoch 12\n",
            "Neural Network training completed in 0.15 seconds\n",
            "Model weights:\n",
            "catboost: 0.4778\n",
            "lightgbm: 0.3726\n",
            "nn: 0.1495\n",
            "[I 2025-02-24 12:22:59,075] Trial 16 finished with value: 0.07790023835627866 and parameters: {'catboost_iterations': 347, 'catboost_lr': 0.06303183029015919, 'catboost_depth': 10, 'catboost_l2_leaf_reg': 9, 'lgbm_n_estimators': 603, 'lgbm_lr': 0.08595507802251393, 'lgbm_num_leaves': 40, 'lgbm_feature_fraction': 0.5854849414574744, 'lgbm_subsample': 0.9928876018956952, 'nn_hidden_dim1': 66, 'nn_hidden_dim2': 33, 'nn_lr': 0.0016776497660734384, 'nn_batch_size': 64, 'nn_epochs': 15}. Best is trial 11 with value: 0.06863991215552195.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 4.35 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.07 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1657, Val Loss = 0.0266\n",
            "Epoch 10: Train Loss = 0.0979, Val Loss = 0.0110\n",
            "Epoch 20: Train Loss = 0.0876, Val Loss = 0.0114\n",
            "Neural Network training completed in 0.80 seconds\n",
            "Model weights:\n",
            "catboost: 0.3843\n",
            "lightgbm: 0.3886\n",
            "nn: 0.2271\n",
            "[I 2025-02-24 12:23:04,372] Trial 17 finished with value: 0.07169143746269983 and parameters: {'catboost_iterations': 543, 'catboost_lr': 0.09906777266281006, 'catboost_depth': 5, 'catboost_l2_leaf_reg': 7, 'lgbm_n_estimators': 322, 'lgbm_lr': 0.00982527563465388, 'lgbm_num_leaves': 58, 'lgbm_feature_fraction': 0.6654246466760864, 'lgbm_subsample': 0.5664365971276084, 'nn_hidden_dim1': 92, 'nn_hidden_dim2': 57, 'nn_lr': 0.00013971511248588763, 'nn_batch_size': 16, 'nn_epochs': 22}. Best is trial 11 with value: 0.06863991215552195.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 0.75 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.09 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1759, Val Loss = 0.0238\n",
            "Epoch 10: Train Loss = 0.0228, Val Loss = 0.0178\n",
            "Epoch 20: Train Loss = 0.0097, Val Loss = 0.0057\n",
            "Early stopping at epoch 26\n",
            "Neural Network training completed in 0.29 seconds\n",
            "Model weights:\n",
            "catboost: 0.3515\n",
            "lightgbm: 0.3097\n",
            "nn: 0.3389\n",
            "[I 2025-02-24 12:23:05,600] Trial 18 finished with value: 0.07121042018130412 and parameters: {'catboost_iterations': 288, 'catboost_lr': 0.023067548514869224, 'catboost_depth': 4, 'catboost_l2_leaf_reg': 3, 'lgbm_n_estimators': 417, 'lgbm_lr': 0.020726475851824164, 'lgbm_num_leaves': 46, 'lgbm_feature_fraction': 0.7994365079527486, 'lgbm_subsample': 0.7043510172911008, 'nn_hidden_dim1': 112, 'nn_hidden_dim2': 36, 'nn_lr': 0.0035761327859255318, 'nn_batch_size': 64, 'nn_epochs': 32}. Best is trial 11 with value: 0.06863991215552195.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 4.48 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.17 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.2034, Val Loss = 0.0075\n",
            "Epoch 10: Train Loss = 0.0372, Val Loss = 0.0159\n",
            "Epoch 20: Train Loss = 0.0233, Val Loss = 0.0048\n",
            "Epoch 30: Train Loss = 0.0127, Val Loss = 0.0049\n",
            "Early stopping at epoch 32\n",
            "Neural Network training completed in 0.64 seconds\n",
            "Model weights:\n",
            "catboost: 0.3109\n",
            "lightgbm: 0.2646\n",
            "nn: 0.4244\n",
            "[I 2025-02-24 12:23:10,984] Trial 19 finished with value: 0.06729435587081709 and parameters: {'catboost_iterations': 748, 'catboost_lr': 0.05731462056236438, 'catboost_depth': 3, 'catboost_l2_leaf_reg': 9, 'lgbm_n_estimators': 770, 'lgbm_lr': 0.054096334336031796, 'lgbm_num_leaves': 53, 'lgbm_feature_fraction': 0.736908386413922, 'lgbm_subsample': 0.6442162186856468, 'nn_hidden_dim1': 42, 'nn_hidden_dim2': 87, 'nn_lr': 0.0010365440648387567, 'nn_batch_size': 32, 'nn_epochs': 40}. Best is trial 19 with value: 0.06729435587081709.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0673\n",
            "\n",
            "Best trial:\n",
            "  RMSE: 0.0673\n",
            "  Best hyperparameters:\n",
            "    catboost_iterations: 748\n",
            "    catboost_lr: 0.05731462056236438\n",
            "    catboost_depth: 3\n",
            "    catboost_l2_leaf_reg: 9\n",
            "    lgbm_n_estimators: 770\n",
            "    lgbm_lr: 0.054096334336031796\n",
            "    lgbm_num_leaves: 53\n",
            "    lgbm_feature_fraction: 0.736908386413922\n",
            "    lgbm_subsample: 0.6442162186856468\n",
            "    nn_hidden_dim1: 42\n",
            "    nn_hidden_dim2: 87\n",
            "    nn_lr: 0.0010365440648387567\n",
            "    nn_batch_size: 32\n",
            "    nn_epochs: 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import optuna\n",
        "import numpy as np\n",
        "\n",
        "# Choose one stock for tuning (CVNA US Equity)\n",
        "stock = 'CVNA US Equity'\n",
        "X_train = X_train_dict[stock]\n",
        "y_train = y_train_dict[stock]\n",
        "X_val = X_val_dict[stock]\n",
        "y_val = y_val_dict[stock]\n",
        "\n",
        "def objective(trial):\n",
        "    # Hyperparameters for CatBoost\n",
        "    catboost_params = {\n",
        "         'iterations': trial.suggest_int('catboost_iterations', 200, 1000),\n",
        "         'learning_rate': trial.suggest_loguniform('catboost_lr', 0.001, 0.1),\n",
        "         'depth': trial.suggest_int('catboost_depth', 3, 10),\n",
        "         'l2_leaf_reg': trial.suggest_int('catboost_l2_leaf_reg', 1, 10),\n",
        "         'verbose': False,\n",
        "    }\n",
        "\n",
        "    # Hyperparameters for LightGBM\n",
        "    lightgbm_params = {\n",
        "         'n_estimators': trial.suggest_int('lgbm_n_estimators', 200, 1000),\n",
        "         'learning_rate': trial.suggest_loguniform('lgbm_lr', 0.001, 0.1),\n",
        "         'num_leaves': trial.suggest_int('lgbm_num_leaves', 8, 64),\n",
        "         'feature_fraction': trial.suggest_uniform('lgbm_feature_fraction', 0.5, 1.0),\n",
        "         'subsample': trial.suggest_uniform('lgbm_subsample', 0.5, 1.0),\n",
        "         'verbose': -1,\n",
        "    }\n",
        "\n",
        "    # Hyperparameters for the Neural Network\n",
        "    nn_hidden_dims = [trial.suggest_int('nn_hidden_dim1', 16, 128),\n",
        "                      trial.suggest_int('nn_hidden_dim2', 16, 128)]\n",
        "    nn_params = {\n",
        "         'hidden_dims': nn_hidden_dims,\n",
        "         'lr': trial.suggest_loguniform('nn_lr', 1e-4, 1e-2),\n",
        "         'batch_size': trial.suggest_categorical('nn_batch_size', [16, 32, 64]),\n",
        "         'epochs': trial.suggest_int('nn_epochs', 10, 50),\n",
        "    }\n",
        "\n",
        "    # Combine hyperparameters into a single dictionary\n",
        "    model_params = {\n",
        "         'catboost': catboost_params,\n",
        "         'lightgbm': lightgbm_params,\n",
        "         'nn': nn_params,\n",
        "    }\n",
        "\n",
        "    # Initialize the ensemble model with these hyperparameters\n",
        "    model = ModelEnsemble(input_dim=X_train.shape[1], model_params=model_params)\n",
        "\n",
        "    # Train the model on the training set and validate on the validation set\n",
        "    model.train(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Evaluate on the validation set using RMSE as our metric\n",
        "    metrics = model.evaluate(X_val, y_val)\n",
        "    rmse = metrics['rmse']\n",
        "    print(f\"Trial RMSE: {rmse:.4f}\")\n",
        "    return rmse\n",
        "\n",
        "# Create an Optuna study to minimize the RMSE\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=20)  # You can increase n_trials for more exhaustive search\n",
        "\n",
        "# Display the best trial results\n",
        "print(\"\\nBest trial:\")\n",
        "best_trial = study.best_trial\n",
        "print(f\"  RMSE: {best_trial.value:.4f}\")\n",
        "print(\"  Best hyperparameters:\")\n",
        "for key, value in best_trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGNC9q228w3e",
        "outputId": "f6a198b1-2486-45ff-94fd-3372a0c75c61"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-24 12:53:18,568] A new study created in memory with name: no-name-174ae47b-d272-43a6-b1e4-2bdcfb126dc1\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n",
            "CatBoost training completed in 2.71 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.09 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1932, Val Loss = 0.0345\n",
            "Early stopping at epoch 10\n",
            "Neural Network training completed in 0.12 seconds\n",
            "Model weights:\n",
            "catboost: 0.6947\n",
            "lightgbm: 0.2516\n",
            "nn: 0.0537\n",
            "[I 2025-02-24 12:53:21,515] Trial 0 finished with value: 0.09034891875427689 and parameters: {'catboost_iterations': 504, 'catboost_lr': 0.09015966317750747, 'catboost_depth': 3, 'catboost_l2_leaf_reg': 10, 'lgbm_n_estimators': 361, 'lgbm_lr': 0.060658020306381066, 'lgbm_num_leaves': 25, 'lgbm_feature_fraction': 0.9820312373190984, 'lgbm_subsample': 0.7781494726957051, 'nn_hidden_dim1': 76, 'nn_hidden_dim2': 111, 'nn_lr': 0.0003853256087849567, 'nn_batch_size': 64, 'nn_epochs': 17}. Best is trial 0 with value: 0.09034891875427689.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 3.93 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.27 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1299, Val Loss = 0.0194\n",
            "Epoch 10: Train Loss = 0.0129, Val Loss = 0.0157\n",
            "Epoch 20: Train Loss = 0.0111, Val Loss = 0.0138\n",
            "Early stopping at epoch 23\n",
            "Neural Network training completed in 0.46 seconds\n",
            "Model weights:\n",
            "catboost: 0.5384\n",
            "lightgbm: 0.2733\n",
            "nn: 0.1884\n",
            "[I 2025-02-24 12:53:26,202] Trial 1 finished with value: 0.10074807613859713 and parameters: {'catboost_iterations': 530, 'catboost_lr': 0.02029895238234372, 'catboost_depth': 4, 'catboost_l2_leaf_reg': 1, 'lgbm_n_estimators': 991, 'lgbm_lr': 0.0039208378752274755, 'lgbm_num_leaves': 43, 'lgbm_feature_fraction': 0.7473307209610671, 'lgbm_subsample': 0.5890004464916847, 'nn_hidden_dim1': 76, 'nn_hidden_dim2': 73, 'nn_lr': 0.005117062276647225, 'nn_batch_size': 32, 'nn_epochs': 29}. Best is trial 0 with value: 0.09034891875427689.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.1007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 1.15 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.18 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1325, Val Loss = 0.0252\n",
            "Early stopping at epoch 10\n",
            "Neural Network training completed in 0.13 seconds\n",
            "Model weights:\n",
            "catboost: 0.5575\n",
            "lightgbm: 0.3935\n",
            "nn: 0.0490\n",
            "[I 2025-02-24 12:53:27,684] Trial 2 finished with value: 0.09307115187306822 and parameters: {'catboost_iterations': 354, 'catboost_lr': 0.006745190855970527, 'catboost_depth': 4, 'catboost_l2_leaf_reg': 1, 'lgbm_n_estimators': 984, 'lgbm_lr': 0.0013894027303148833, 'lgbm_num_leaves': 24, 'lgbm_feature_fraction': 0.507968014341379, 'lgbm_subsample': 0.844561096733665, 'nn_hidden_dim1': 116, 'nn_hidden_dim2': 22, 'nn_lr': 0.0002936575248546899, 'nn_batch_size': 64, 'nn_epochs': 34}. Best is trial 0 with value: 0.09034891875427689.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0931\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 5.03 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.07 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1474, Val Loss = 0.0560\n",
            "Epoch 10: Train Loss = 0.0363, Val Loss = 0.0242\n",
            "Epoch 20: Train Loss = 0.0229, Val Loss = 0.0147\n",
            "Early stopping at epoch 29\n",
            "Neural Network training completed in 0.58 seconds\n",
            "Model weights:\n",
            "catboost: 0.4454\n",
            "lightgbm: 0.3833\n",
            "nn: 0.1713\n",
            "[I 2025-02-24 12:53:33,382] Trial 3 finished with value: 0.09661207382031603 and parameters: {'catboost_iterations': 800, 'catboost_lr': 0.0011725445625753086, 'catboost_depth': 3, 'catboost_l2_leaf_reg': 9, 'lgbm_n_estimators': 327, 'lgbm_lr': 0.0020654196926751427, 'lgbm_num_leaves': 61, 'lgbm_feature_fraction': 0.6781431674446329, 'lgbm_subsample': 0.7029331740072958, 'nn_hidden_dim1': 35, 'nn_hidden_dim2': 96, 'nn_lr': 0.00141400831474596, 'nn_batch_size': 32, 'nn_epochs': 48}. Best is trial 0 with value: 0.09034891875427689.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 28.14 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.06 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.3707, Val Loss = 0.0879\n",
            "Epoch 10: Train Loss = 0.1298, Val Loss = 0.0460\n",
            "Early stopping at epoch 16\n",
            "Neural Network training completed in 0.61 seconds\n",
            "Model weights:\n",
            "catboost: 0.6352\n",
            "lightgbm: 0.2952\n",
            "nn: 0.0696\n",
            "[I 2025-02-24 12:54:02,225] Trial 4 finished with value: 0.09889215459564134 and parameters: {'catboost_iterations': 534, 'catboost_lr': 0.006502542946303938, 'catboost_depth': 10, 'catboost_l2_leaf_reg': 1, 'lgbm_n_estimators': 270, 'lgbm_lr': 0.017882601166847943, 'lgbm_num_leaves': 18, 'lgbm_feature_fraction': 0.8609758275721384, 'lgbm_subsample': 0.6589142208782826, 'nn_hidden_dim1': 104, 'nn_hidden_dim2': 70, 'nn_lr': 0.0001599940531493844, 'nn_batch_size': 16, 'nn_epochs': 32}. Best is trial 0 with value: 0.09034891875427689.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 1.28 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.09 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1452, Val Loss = 0.0433\n",
            "Epoch 10: Train Loss = 0.0557, Val Loss = 0.0260\n",
            "Neural Network training completed in 0.23 seconds\n",
            "Model weights:\n",
            "catboost: 0.5049\n",
            "lightgbm: 0.3460\n",
            "nn: 0.1491\n",
            "[I 2025-02-24 12:54:03,850] Trial 5 finished with value: 0.09672702822559426 and parameters: {'catboost_iterations': 358, 'catboost_lr': 0.0667971352797028, 'catboost_depth': 5, 'catboost_l2_leaf_reg': 2, 'lgbm_n_estimators': 471, 'lgbm_lr': 0.00323915751984368, 'lgbm_num_leaves': 8, 'lgbm_feature_fraction': 0.9321233860539335, 'lgbm_subsample': 0.7073921667654861, 'nn_hidden_dim1': 116, 'nn_hidden_dim2': 27, 'nn_lr': 0.0006188433507397209, 'nn_batch_size': 32, 'nn_epochs': 11}. Best is trial 0 with value: 0.09034891875427689.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 5.06 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.20 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1373, Val Loss = 0.0183\n",
            "Epoch 10: Train Loss = 0.0174, Val Loss = 0.0116\n",
            "Early stopping at epoch 14\n",
            "Neural Network training completed in 0.30 seconds\n",
            "Model weights:\n",
            "catboost: 0.4876\n",
            "lightgbm: 0.2585\n",
            "nn: 0.2539\n",
            "[I 2025-02-24 12:54:09,428] Trial 6 finished with value: 0.09799635799455843 and parameters: {'catboost_iterations': 501, 'catboost_lr': 0.08218604430620977, 'catboost_depth': 6, 'catboost_l2_leaf_reg': 10, 'lgbm_n_estimators': 882, 'lgbm_lr': 0.003204357617354077, 'lgbm_num_leaves': 49, 'lgbm_feature_fraction': 0.6536293427856223, 'lgbm_subsample': 0.7295088480563613, 'nn_hidden_dim1': 31, 'nn_hidden_dim2': 73, 'nn_lr': 0.002916886206925939, 'nn_batch_size': 32, 'nn_epochs': 17}. Best is trial 0 with value: 0.09034891875427689.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 1.98 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.17 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1402, Val Loss = 0.0119\n",
            "Epoch 10: Train Loss = 0.0138, Val Loss = 0.0097\n",
            "Early stopping at epoch 12\n",
            "Neural Network training completed in 0.27 seconds\n",
            "Model weights:\n",
            "catboost: 0.4411\n",
            "lightgbm: 0.2127\n",
            "nn: 0.3462\n",
            "[I 2025-02-24 12:54:11,871] Trial 7 finished with value: 0.09404250958884249 and parameters: {'catboost_iterations': 339, 'catboost_lr': 0.040832591973729515, 'catboost_depth': 9, 'catboost_l2_leaf_reg': 7, 'lgbm_n_estimators': 830, 'lgbm_lr': 0.012200271732353364, 'lgbm_num_leaves': 62, 'lgbm_feature_fraction': 0.5340514900238924, 'lgbm_subsample': 0.9055390207489318, 'nn_hidden_dim1': 45, 'nn_hidden_dim2': 16, 'nn_lr': 0.007120687505523447, 'nn_batch_size': 32, 'nn_epochs': 41}. Best is trial 0 with value: 0.09034891875427689.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 10.97 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.11 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1839, Val Loss = 0.1066\n",
            "Epoch 10: Train Loss = 0.1132, Val Loss = 0.0401\n",
            "Epoch 20: Train Loss = 0.0950, Val Loss = 0.0286\n",
            "Epoch 30: Train Loss = 0.0756, Val Loss = 0.0268\n",
            "Early stopping at epoch 34\n",
            "Neural Network training completed in 0.68 seconds\n",
            "Model weights:\n",
            "catboost: 0.5606\n",
            "lightgbm: 0.2703\n",
            "nn: 0.1691\n",
            "[I 2025-02-24 12:54:23,647] Trial 8 finished with value: 0.0965832949986262 and parameters: {'catboost_iterations': 903, 'catboost_lr': 0.04939287830956901, 'catboost_depth': 7, 'catboost_l2_leaf_reg': 5, 'lgbm_n_estimators': 544, 'lgbm_lr': 0.015357625237649311, 'lgbm_num_leaves': 15, 'lgbm_feature_fraction': 0.5964713144752591, 'lgbm_subsample': 0.7385866188448378, 'nn_hidden_dim1': 77, 'nn_hidden_dim2': 30, 'nn_lr': 0.000108397429496404, 'nn_batch_size': 32, 'nn_epochs': 44}. Best is trial 0 with value: 0.09034891875427689.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 0.93 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.08 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1198, Val Loss = 0.0345\n",
            "Epoch 10: Train Loss = 0.0143, Val Loss = 0.0248\n",
            "Epoch 20: Train Loss = 0.0114, Val Loss = 0.0163\n",
            "Early stopping at epoch 26\n",
            "Neural Network training completed in 0.52 seconds\n",
            "Model weights:\n",
            "catboost: 0.5320\n",
            "lightgbm: 0.2753\n",
            "nn: 0.1926\n",
            "[I 2025-02-24 12:54:25,198] Trial 9 finished with value: 0.09851134310660431 and parameters: {'catboost_iterations': 292, 'catboost_lr': 0.004415738424814845, 'catboost_depth': 4, 'catboost_l2_leaf_reg': 7, 'lgbm_n_estimators': 390, 'lgbm_lr': 0.010652629208935048, 'lgbm_num_leaves': 19, 'lgbm_feature_fraction': 0.610697805755857, 'lgbm_subsample': 0.7520186915372801, 'nn_hidden_dim1': 50, 'nn_hidden_dim2': 18, 'nn_lr': 0.004464697191240058, 'nn_batch_size': 32, 'nn_epochs': 38}. Best is trial 0 with value: 0.09034891875427689.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 7.87 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.17 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.3748, Val Loss = 0.0538\n",
            "Epoch 10: Train Loss = 0.0797, Val Loss = 0.0152\n",
            "Epoch 20: Train Loss = 0.0668, Val Loss = 0.0174\n",
            "Early stopping at epoch 21\n",
            "Neural Network training completed in 0.25 seconds\n",
            "Model weights:\n",
            "catboost: 0.5115\n",
            "lightgbm: 0.2168\n",
            "nn: 0.2717\n",
            "[I 2025-02-24 12:54:33,564] Trial 10 finished with value: 0.08226176855170557 and parameters: {'catboost_iterations': 653, 'catboost_lr': 0.018707257898840212, 'catboost_depth': 7, 'catboost_l2_leaf_reg': 4, 'lgbm_n_estimators': 695, 'lgbm_lr': 0.09689471414279523, 'lgbm_num_leaves': 32, 'lgbm_feature_fraction': 0.9738609972079354, 'lgbm_subsample': 0.9775035686836503, 'nn_hidden_dim1': 91, 'nn_hidden_dim2': 126, 'nn_lr': 0.0007329415535125036, 'nn_batch_size': 64, 'nn_epochs': 23}. Best is trial 10 with value: 0.08226176855170557.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 12.60 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.16 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1917, Val Loss = 0.0344\n",
            "Early stopping at epoch 10\n",
            "Neural Network training completed in 0.12 seconds\n",
            "Model weights:\n",
            "catboost: 0.6993\n",
            "lightgbm: 0.2713\n",
            "nn: 0.0294\n",
            "[I 2025-02-24 12:54:46,520] Trial 11 finished with value: 0.09926800555318613 and parameters: {'catboost_iterations': 694, 'catboost_lr': 0.019297209167587058, 'catboost_depth': 8, 'catboost_l2_leaf_reg': 4, 'lgbm_n_estimators': 671, 'lgbm_lr': 0.09423084185921196, 'lgbm_num_leaves': 31, 'lgbm_feature_fraction': 0.9974482486913674, 'lgbm_subsample': 0.9980261764404209, 'nn_hidden_dim1': 87, 'nn_hidden_dim2': 126, 'nn_lr': 0.0005580936312232674, 'nn_batch_size': 64, 'nn_epochs': 22}. Best is trial 10 with value: 0.08226176855170557.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 8.53 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.15 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.2059, Val Loss = 0.0139\n",
            "Epoch 10: Train Loss = 0.0612, Val Loss = 0.0116\n",
            "Early stopping at epoch 11\n",
            "Neural Network training completed in 0.14 seconds\n",
            "Model weights:\n",
            "catboost: 0.5553\n",
            "lightgbm: 0.1967\n",
            "nn: 0.2480\n",
            "[I 2025-02-24 12:54:55,433] Trial 12 finished with value: 0.0808239851146129 and parameters: {'catboost_iterations': 677, 'catboost_lr': 0.02108262128963427, 'catboost_depth': 7, 'catboost_l2_leaf_reg': 4, 'lgbm_n_estimators': 668, 'lgbm_lr': 0.09671409581418867, 'lgbm_num_leaves': 33, 'lgbm_feature_fraction': 0.8628918652898527, 'lgbm_subsample': 0.9878992985131347, 'nn_hidden_dim1': 96, 'nn_hidden_dim2': 124, 'nn_lr': 0.0013365869940667667, 'nn_batch_size': 64, 'nn_epochs': 21}. Best is trial 12 with value: 0.0808239851146129.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 8.70 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.16 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1471, Val Loss = 0.0452\n",
            "Epoch 10: Train Loss = 0.0569, Val Loss = 0.1471\n",
            "Early stopping at epoch 11\n",
            "Neural Network training completed in 0.14 seconds\n",
            "Model weights:\n",
            "catboost: 0.6742\n",
            "lightgbm: 0.2760\n",
            "nn: 0.0498\n",
            "[I 2025-02-24 12:55:04,507] Trial 13 finished with value: 0.10064332042768974 and parameters: {'catboost_iterations': 698, 'catboost_lr': 0.01724922271838479, 'catboost_depth': 7, 'catboost_l2_leaf_reg': 3, 'lgbm_n_estimators': 693, 'lgbm_lr': 0.038788439932419795, 'lgbm_num_leaves': 39, 'lgbm_feature_fraction': 0.8575167431971824, 'lgbm_subsample': 0.9385345127780168, 'nn_hidden_dim1': 96, 'nn_hidden_dim2': 125, 'nn_lr': 0.0014378338505395512, 'nn_batch_size': 64, 'nn_epochs': 26}. Best is trial 12 with value: 0.0808239851146129.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.1006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 17.59 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.16 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1860, Val Loss = 0.0211\n",
            "Epoch 10: Train Loss = 0.0313, Val Loss = 0.0215\n",
            "Epoch 20: Train Loss = 0.0171, Val Loss = 0.0395\n",
            "Early stopping at epoch 21\n",
            "Neural Network training completed in 0.25 seconds\n",
            "Model weights:\n",
            "catboost: 0.6273\n",
            "lightgbm: 0.2401\n",
            "nn: 0.1326\n",
            "[I 2025-02-24 12:55:22,590] Trial 14 finished with value: 0.10305683383529371 and parameters: {'catboost_iterations': 958, 'catboost_lr': 0.026670276938519227, 'catboost_depth': 8, 'catboost_l2_leaf_reg': 6, 'lgbm_n_estimators': 745, 'lgbm_lr': 0.02964067347446488, 'lgbm_num_leaves': 31, 'lgbm_feature_fraction': 0.8654954547874218, 'lgbm_subsample': 0.998491133686906, 'nn_hidden_dim1': 100, 'nn_hidden_dim2': 99, 'nn_lr': 0.002182358954084866, 'nn_batch_size': 64, 'nn_epochs': 23}. Best is trial 12 with value: 0.0808239851146129.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.1031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 6.18 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.14 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1809, Val Loss = 0.0536\n",
            "Neural Network training completed in 0.40 seconds\n",
            "Model weights:\n",
            "catboost: 0.5844\n",
            "lightgbm: 0.2366\n",
            "nn: 0.1790\n",
            "[I 2025-02-24 12:55:29,398] Trial 15 finished with value: 0.09469796999831838 and parameters: {'catboost_iterations': 682, 'catboost_lr': 0.011427979406484795, 'catboost_depth': 6, 'catboost_l2_leaf_reg': 4, 'lgbm_n_estimators': 573, 'lgbm_lr': 0.09404804477226678, 'lgbm_num_leaves': 47, 'lgbm_feature_fraction': 0.7890968503956759, 'lgbm_subsample': 0.8734379419837783, 'nn_hidden_dim1': 62, 'nn_hidden_dim2': 108, 'nn_lr': 0.0008681203023751772, 'nn_batch_size': 16, 'nn_epochs': 10}. Best is trial 12 with value: 0.0808239851146129.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 14.82 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.19 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1970, Val Loss = 0.0159\n",
            "Epoch 10: Train Loss = 0.0577, Val Loss = 0.0454\n",
            "Early stopping at epoch 11\n",
            "Neural Network training completed in 0.13 seconds\n",
            "Model weights:\n",
            "catboost: 0.6276\n",
            "lightgbm: 0.2502\n",
            "nn: 0.1222\n",
            "[I 2025-02-24 12:55:44,631] Trial 16 finished with value: 0.10212525858677558 and parameters: {'catboost_iterations': 830, 'catboost_lr': 0.0028650769647622426, 'catboost_depth': 8, 'catboost_l2_leaf_reg': 3, 'lgbm_n_estimators': 807, 'lgbm_lr': 0.04361757996535909, 'lgbm_num_leaves': 33, 'lgbm_feature_fraction': 0.9327637466994689, 'lgbm_subsample': 0.5199291512531907, 'nn_hidden_dim1': 126, 'nn_hidden_dim2': 54, 'nn_lr': 0.001094784709118896, 'nn_batch_size': 64, 'nn_epochs': 17}. Best is trial 12 with value: 0.0808239851146129.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.1021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 32.81 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.14 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.3411, Val Loss = 0.1115\n",
            "Epoch 10: Train Loss = 0.1140, Val Loss = 0.2271\n",
            "Early stopping at epoch 13\n",
            "Neural Network training completed in 0.16 seconds\n",
            "Model weights:\n",
            "catboost: 0.6730\n",
            "lightgbm: 0.3001\n",
            "nn: 0.0269\n",
            "[I 2025-02-24 12:56:17,823] Trial 17 finished with value: 0.09660981979929215 and parameters: {'catboost_iterations': 619, 'catboost_lr': 0.03462341548430671, 'catboost_depth': 10, 'catboost_l2_leaf_reg': 6, 'lgbm_n_estimators': 601, 'lgbm_lr': 0.025496555940614965, 'lgbm_num_leaves': 53, 'lgbm_feature_fraction': 0.7893846460398298, 'lgbm_subsample': 0.8321026230994807, 'nn_hidden_dim1': 88, 'nn_hidden_dim2': 112, 'nn_lr': 0.00027050273475635135, 'nn_batch_size': 64, 'nn_epochs': 21}. Best is trial 12 with value: 0.0808239851146129.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 7.13 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.10 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1712, Val Loss = 0.0114\n",
            "Early stopping at epoch 10\n",
            "Neural Network training completed in 0.13 seconds\n",
            "Model weights:\n",
            "catboost: 0.6005\n",
            "lightgbm: 0.2948\n",
            "nn: 0.1047\n",
            "[I 2025-02-24 12:56:25,261] Trial 18 finished with value: 0.10334438287379667 and parameters: {'catboost_iterations': 785, 'catboost_lr': 0.011499685234579957, 'catboost_depth': 6, 'catboost_l2_leaf_reg': 4, 'lgbm_n_estimators': 465, 'lgbm_lr': 0.006259280612844082, 'lgbm_num_leaves': 37, 'lgbm_feature_fraction': 0.9105837322273953, 'lgbm_subsample': 0.9444718246019856, 'nn_hidden_dim1': 18, 'nn_hidden_dim2': 86, 'nn_lr': 0.0021454674024577692, 'nn_batch_size': 64, 'nn_epochs': 26}. Best is trial 12 with value: 0.0808239851146129.\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.1033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 18.17 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.15 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1873, Val Loss = 0.0242\n",
            "Early stopping at epoch 10\n",
            "Neural Network training completed in 0.41 seconds\n",
            "Model weights:\n",
            "catboost: 0.5892\n",
            "lightgbm: 0.2437\n",
            "nn: 0.1671\n",
            "[I 2025-02-24 12:56:44,071] Trial 19 finished with value: 0.09087958384270685 and parameters: {'catboost_iterations': 600, 'catboost_lr': 0.0016381505336477136, 'catboost_depth': 9, 'catboost_l2_leaf_reg': 8, 'lgbm_n_estimators': 660, 'lgbm_lr': 0.05485627740412291, 'lgbm_num_leaves': 27, 'lgbm_feature_fraction': 0.8055113019922979, 'lgbm_subsample': 0.9519203252119387, 'nn_hidden_dim1': 63, 'nn_hidden_dim2': 125, 'nn_lr': 0.000629957471383939, 'nn_batch_size': 16, 'nn_epochs': 35}. Best is trial 12 with value: 0.0808239851146129.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial RMSE: 0.0909\n",
            "\n",
            "Best trial:\n",
            "  RMSE: 0.0808\n",
            "  Best hyperparameters:\n",
            "    catboost_iterations: 677\n",
            "    catboost_lr: 0.02108262128963427\n",
            "    catboost_depth: 7\n",
            "    catboost_l2_leaf_reg: 4\n",
            "    lgbm_n_estimators: 668\n",
            "    lgbm_lr: 0.09671409581418867\n",
            "    lgbm_num_leaves: 33\n",
            "    lgbm_feature_fraction: 0.8628918652898527\n",
            "    lgbm_subsample: 0.9878992985131347\n",
            "    nn_hidden_dim1: 96\n",
            "    nn_hidden_dim2: 124\n",
            "    nn_lr: 0.0013365869940667667\n",
            "    nn_batch_size: 64\n",
            "    nn_epochs: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate_best_params_across_stocks(best_params, X_train_dict, y_train_dict, X_val_dict, y_val_dict):\n",
        "    results = {}\n",
        "    # Prepare the combined model parameters dictionary using the best hyperparameters\n",
        "    model_params = {\n",
        "         'catboost': {\n",
        "             'iterations': best_params['catboost_iterations'],\n",
        "             'learning_rate': best_params['catboost_lr'],\n",
        "             'depth': best_params['catboost_depth'],\n",
        "             'l2_leaf_reg': best_params['catboost_l2_leaf_reg'],\n",
        "             'verbose': False,\n",
        "         },\n",
        "         'lightgbm': {\n",
        "             'n_estimators': best_params['lgbm_n_estimators'],\n",
        "             'learning_rate': best_params['lgbm_lr'],\n",
        "             'num_leaves': best_params['lgbm_num_leaves'],\n",
        "             'feature_fraction': best_params['lgbm_feature_fraction'],\n",
        "             'subsample': best_params['lgbm_subsample'],\n",
        "             'verbose': -1,\n",
        "         },\n",
        "         'nn': {\n",
        "             'hidden_dims': [best_params['nn_hidden_dim1'], best_params['nn_hidden_dim2']],\n",
        "             'lr': best_params['nn_lr'],\n",
        "             'batch_size': best_params['nn_batch_size'],\n",
        "             'epochs': best_params['nn_epochs'],\n",
        "         }\n",
        "    }\n",
        "\n",
        "    for stock in X_train_dict.keys():\n",
        "        print(f\"\\nEvaluating stock: {stock}\")\n",
        "        X_train = X_train_dict[stock]\n",
        "        y_train = y_train_dict[stock]\n",
        "        X_val = X_val_dict[stock]\n",
        "        y_val = y_val_dict[stock]\n",
        "\n",
        "        model = ModelEnsemble(input_dim=X_train.shape[1], model_params=model_params)\n",
        "        model.train(X_train, y_train, X_val, y_val)\n",
        "        metrics = model.evaluate(X_val, y_val)\n",
        "        results[stock] = metrics\n",
        "        print(f\"{stock} - Validation RMSE: {metrics['rmse']:.4f}\")\n",
        "    return results\n",
        "\n",
        "# Use the best parameters from the tuning step (from study.best_trial.params)\n",
        "best_hyperparams = study.best_trial.params\n",
        "\n",
        "# Evaluate these best parameters across all stocks\n",
        "all_results = evaluate_best_params_across_stocks(best_hyperparams, X_train_dict, y_train_dict, X_val_dict, y_val_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wjugpju8ypy",
        "outputId": "efb6bd8a-0332-4d29-fe8d-acdfc946ab13"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating stock: ALT US Equity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 7.85 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.15 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.2134, Val Loss = 0.0185\n",
            "Epoch 10: Train Loss = 0.0808, Val Loss = 0.0207\n",
            "Epoch 20: Train Loss = 0.0361, Val Loss = 0.0154\n",
            "Neural Network training completed in 0.24 seconds\n",
            "Model weights:\n",
            "catboost: 0.4530\n",
            "lightgbm: 0.2631\n",
            "nn: 0.2839\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALT US Equity - Validation RMSE: 0.1029\n",
            "\n",
            "Evaluating stock: CELH US Equity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 8.41 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.18 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.2166, Val Loss = 0.0627\n",
            "Epoch 10: Train Loss = 0.0741, Val Loss = 0.0088\n",
            "Early stopping at epoch 19\n",
            "Neural Network training completed in 0.22 seconds\n",
            "Model weights:\n",
            "catboost: 0.4407\n",
            "lightgbm: 0.3404\n",
            "nn: 0.2189\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CELH US Equity - Validation RMSE: 0.0714\n",
            "\n",
            "Evaluating stock: CVNA US Equity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 8.49 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.16 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1718, Val Loss = 0.0281\n",
            "Early stopping at epoch 10\n",
            "Neural Network training completed in 0.12 seconds\n",
            "Model weights:\n",
            "catboost: 0.6257\n",
            "lightgbm: 0.2216\n",
            "nn: 0.1527\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CVNA US Equity - Validation RMSE: 0.1012\n",
            "\n",
            "Evaluating stock: FUBO US Equity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 8.63 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.16 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1597, Val Loss = 0.7969\n",
            "Epoch 10: Train Loss = 0.0523, Val Loss = 2.3933\n",
            "Early stopping at epoch 18\n",
            "Neural Network training completed in 0.22 seconds\n",
            "Model weights:\n",
            "catboost: 0.4303\n",
            "lightgbm: 0.4425\n",
            "nn: 0.1272\n",
            "CatBoost will use GPU\n",
            "LightGBM will use CPU for stability\n",
            "Starting model training...\n",
            "\n",
            "Training CatBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FUBO US Equity - Validation RMSE: 0.8381\n",
            "\n",
            "Evaluating stock: UPST US Equity\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CatBoost training completed in 8.52 seconds\n",
            "\n",
            "Training LightGBM...\n",
            "LightGBM training completed in 0.16 seconds\n",
            "\n",
            "Training Neural Network...\n",
            "Epoch 0: Train Loss = 0.1348, Val Loss = 0.0538\n",
            "Epoch 10: Train Loss = 0.0493, Val Loss = 0.1938\n",
            "Early stopping at epoch 11\n",
            "Neural Network training completed in 0.13 seconds\n",
            "Model weights:\n",
            "catboost: 0.4470\n",
            "lightgbm: 0.4157\n",
            "nn: 0.1374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UPST US Equity - Validation RMSE: 0.1533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_target_distribution(df):\n",
        "    stocks = df['Stock'].unique()\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i, stock in enumerate(stocks, 1):\n",
        "        stock_df = df[df['Stock'] == stock].copy()\n",
        "        # Assuming 'target' was created in prepare_data_for_modeling, if not, we can compute it here:\n",
        "        # For a quick estimate, we define target as the percentage change in 'Last Price'\n",
        "        if 'target' not in stock_df.columns:\n",
        "            stock_df['target'] = stock_df['Last Price'].pct_change().shift(-5)  # 5-day ahead return\n",
        "        plt.subplot(3, 2, i)\n",
        "        plt.hist(stock_df['target'].dropna(), bins=50, color='skyblue', edgecolor='black')\n",
        "        plt.title(f\"Target Distribution for {stock}\")\n",
        "        plt.xlabel(\"Target Return\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run the plot on your processed data\n",
        "plot_target_distribution(processed_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "id": "yUQHhiEY_lb3",
        "outputId": "a5222266-02c1-4eb9-a3c6-503a39c13db9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1000 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying to fix FUBO"
      ],
      "metadata": {
        "id": "P6Ch88pVAA59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def clip_outliers_by_iqr(stock_df, col=\"target\", iqr_factor=1.5):\n",
        "    \"\"\"\n",
        "    Clips values in 'col' outside [Q1 - iqr_factor*IQR, Q3 + iqr_factor*IQR].\n",
        "    \"\"\"\n",
        "    q1 = stock_df[col].quantile(0.25)\n",
        "    q3 = stock_df[col].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - iqr_factor * iqr\n",
        "    upper_bound = q3 + iqr_factor * iqr\n",
        "\n",
        "    # Clip the values\n",
        "    stock_df[col] = np.clip(stock_df[col], lower_bound, upper_bound)\n",
        "    return stock_df\n",
        "\n",
        "def apply_outlier_clipping(df, col=\"target\"):\n",
        "    \"\"\"\n",
        "    Applies IQR-based outlier clipping to each stock in the DataFrame.\n",
        "    \"\"\"\n",
        "    df_clipped = df.copy()\n",
        "    for stock in df_clipped['Stock'].unique():\n",
        "        mask = (df_clipped['Stock'] == stock)\n",
        "        df_clipped.loc[mask] = clip_outliers_by_iqr(df_clipped.loc[mask], col)\n",
        "    return df_clipped\n"
      ],
      "metadata": {
        "id": "AacTA0ua__hD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def clip_outliers_by_iqr(series, iqr_factor=1.5):\n",
        "    \"\"\"\n",
        "    Clips outliers in the series using the Interquartile Range (IQR) method.\n",
        "    \"\"\"\n",
        "    q1 = series.quantile(0.25)\n",
        "    q3 = series.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - iqr_factor * iqr\n",
        "    upper_bound = q3 + iqr_factor * iqr\n",
        "\n",
        "    return np.clip(series, lower_bound, upper_bound)\n",
        "\n",
        "def prepare_data_for_modeling(self, df: pd.DataFrame, window_size: int = 252,\n",
        "                              val_size: int = 63, test_size: int = 21, target_days: int = 5) -> Tuple[Dict[str, np.ndarray], ...]:\n",
        "    \"\"\"\n",
        "    Prepare data for modeling using rolling windows approach.\n",
        "\n",
        "    Args:\n",
        "        df: Processed DataFrame\n",
        "        window_size: Number of days for training window\n",
        "        val_size: Number of days for validation\n",
        "        test_size: Number of days for testing\n",
        "        target_days: Number of days ahead to predict\n",
        "    \"\"\"\n",
        "    feature_cols = [\n",
        "        # Price features\n",
        "        'Last Price', 'Open Price', 'High Price', 'Low Price',\n",
        "        'Moving Average 20 Day', 'Moving Average 50 Day', 'Moving Average 200 Day',\n",
        "\n",
        "        # Volume features\n",
        "        'Volume', 'Average Volume 30 Day',\n",
        "\n",
        "        # Technical indicators\n",
        "        'RSI 14 Day', 'Volatility 30 Day',\n",
        "        'ROC_5', 'ROC_10', 'ROC_20',\n",
        "        'MACD', 'MACD_Signal', 'MACD_Diff',\n",
        "        'BB_High', 'BB_Mid', 'BB_Low', 'BB_Width', 'ATR',\n",
        "        'OBV', 'ADI',\n",
        "\n",
        "        # Derived features\n",
        "        'Daily_Return', 'Log_Return', 'High_Low_Range',\n",
        "        'Price_MA_Ratio_20', 'Price_MA_Ratio_50', 'Price_MA_Ratio_200',\n",
        "        'Distance_To_MA_20', 'Distance_To_MA_50',\n",
        "        'Volume_MA_Ratio', 'Volume_Price_Trend'\n",
        "    ]\n",
        "\n",
        "    X_train_dict = {}\n",
        "    X_val_dict = {}\n",
        "    X_test_dict = {}\n",
        "    y_train_dict = {}\n",
        "    y_val_dict = {}\n",
        "    y_test_dict = {}\n",
        "\n",
        "    for stock in df['Stock'].unique():\n",
        "        # Get data for this stock\n",
        "        stock_df = df[df['Stock'] == stock].copy()\n",
        "        stock_df = stock_df.sort_values('Dates')\n",
        "\n",
        "        # Create target variable (future returns)\n",
        "        stock_df['target'] = stock_df['Last Price'].shift(-target_days) / stock_df['Last Price'] - 1\n",
        "\n",
        "        # **Apply outlier clipping on target before splitting**\n",
        "        stock_df['target'] = clip_outliers_by_iqr(stock_df['target'])\n",
        "\n",
        "        # Get the last window of data\n",
        "        total_size = window_size + val_size + test_size\n",
        "        final_window = stock_df.iloc[-total_size:].copy()\n",
        "\n",
        "        # Split into train/val/test\n",
        "        train_data = final_window.iloc[:window_size]\n",
        "        val_data = final_window.iloc[window_size:window_size + val_size]\n",
        "        test_data = final_window.iloc[window_size + val_size:]\n",
        "\n",
        "        # Scale features using only training data\n",
        "        scaler = StandardScaler()\n",
        "        train_features = scaler.fit_transform(train_data[feature_cols])\n",
        "        val_features = scaler.transform(val_data[feature_cols])\n",
        "        test_features = scaler.transform(test_data[feature_cols])\n",
        "\n",
        "        # **Ensure target is assigned after clipping**\n",
        "        y_train = train_data['target'].fillna(0).values\n",
        "        y_val = val_data['target'].fillna(0).values\n",
        "        y_test = test_data['target'].fillna(0).values\n",
        "\n",
        "        # Store splits\n",
        "        X_train_dict[stock] = train_features\n",
        "        X_val_dict[stock] = val_features\n",
        "        X_test_dict[stock] = test_features\n",
        "        y_train_dict[stock] = y_train\n",
        "        y_val_dict[stock] = y_val\n",
        "        y_test_dict[stock] = y_test\n",
        "\n",
        "        # Print info about the splits\n",
        "        print(f\"\\nSplit info for {stock}:\")\n",
        "        print(f\"Training period: {train_data['Dates'].iloc[0]} to {train_data['Dates'].iloc[-1]}\")\n",
        "        print(f\"Validation period: {val_data['Dates'].iloc[0]} to {val_data['Dates'].iloc[-1]}\")\n",
        "        print(f\"Test period: {test_data['Dates'].iloc[0]} to {test_data['Dates'].iloc[-1]}\")\n",
        "        print(f\"Training samples: {len(train_features)}\")\n",
        "        print(f\"Validation samples: {len(val_features)}\")\n",
        "        print(f\"Test samples: {len(test_features)}\")\n",
        "\n",
        "    return (X_train_dict, X_val_dict, X_test_dict, y_train_dict, y_val_dict, y_test_dict)\n"
      ],
      "metadata": {
        "id": "d410L2G-AFZL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "final_predictions = {}\n",
        "\n",
        "# Loop over each stock's test set\n",
        "for stock in X_test_dict.keys():\n",
        "    # Get the test features for this stock (assumed to be the most recent window)\n",
        "    X_latest = X_test_dict[stock]\n",
        "\n",
        "    pred_returns = model.predict(X_latest)\n",
        "\n",
        "    final_pred_return = pred_returns[-1]  # Using the last prediction\n",
        "\n",
        "    last_price = processed_df[processed_df['Stock'] == stock]['Last Price'].iloc[-1]\n",
        "\n",
        "    # Convert the predicted return to a forecasted price.\n",
        "    predicted_price = last_price * (1 + final_pred_return)\n",
        "\n",
        "    final_predictions[stock] = {\n",
        "        \"predicted_5day_return\": final_pred_return,\n",
        "        \"predicted_price\": predicted_price\n",
        "    }\n",
        "\n",
        "# Print out the final predictions\n",
        "print(\"Final Predictions (for the next 5-day period):\")\n",
        "for stock, preds in final_predictions.items():\n",
        "    print(f\"{stock}: Predicted 5-day return = {preds['predicted_5day_return']:.4f} \"\n",
        "          f\"({preds['predicted_5day_return']*100:.2f}%), \"\n",
        "          f\"Predicted price = ${preds['predicted_price']:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE9MV751DFg-",
        "outputId": "c978d0c7-2d10-4a90-e7fc-132f6364b10a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Predictions (for the next 5-day period):\n",
            "ALT US Equity: Predicted 5-day return = 0.0457 (4.57%), Predicted price = $6.72\n",
            "CELH US Equity: Predicted 5-day return = 0.1609 (16.09%), Predicted price = $37.87\n",
            "CVNA US Equity: Predicted 5-day return = -0.0319 (-3.19%), Predicted price = $216.18\n",
            "FUBO US Equity: Predicted 5-day return = 0.0921 (9.21%), Predicted price = $4.11\n",
            "UPST US Equity: Predicted 5-day return = 0.0128 (1.28%), Predicted price = $72.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results = {}\n",
        "\n",
        "print(\"Evaluation Metrics on Test Sets:\")\n",
        "for stock in X_test_dict.keys():\n",
        "    metrics = model.evaluate(X_test_dict[stock], y_test_dict[stock])\n",
        "    evaluation_results[stock] = metrics\n",
        "    print(f\"{stock}: MAE = {metrics['mae']:.4f}, RMSE = {metrics['rmse']:.4f}, R2 = {metrics['r2']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBQSAKQ2EQ8p",
        "outputId": "2cbdba73-aee6-4742-fc47-53d01f494de7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Metrics on Test Sets:\n",
            "ALT US Equity: MAE = 0.0601, RMSE = 0.0703, R2 = -1.1468\n",
            "CELH US Equity: MAE = 0.2153, RMSE = 0.2271, R2 = -2.1038\n",
            "CVNA US Equity: MAE = 0.0710, RMSE = 0.0782, R2 = -0.4286\n",
            "FUBO US Equity: MAE = 0.0947, RMSE = 0.1110, R2 = -0.7557\n",
            "UPST US Equity: MAE = 0.0968, RMSE = 0.1218, R2 = 0.1349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# === Best hyperparameters from hyperparameter tuning ===\n",
        "best_trial_params = {\n",
        "    \"catboost_iterations\": 677,\n",
        "    \"catboost_lr\": 0.02108262128963427,\n",
        "    \"catboost_depth\": 7,\n",
        "    \"catboost_l2_leaf_reg\": 4,\n",
        "    \"lgbm_n_estimators\": 668,\n",
        "    \"lgbm_lr\": 0.09671409581418867,\n",
        "    \"lgbm_num_leaves\": 33,\n",
        "    \"lgbm_feature_fraction\": 0.8628918652898527,\n",
        "    \"lgbm_subsample\": 0.9878992985131347,\n",
        "    \"nn_hidden_dim1\": 96,\n",
        "    \"nn_hidden_dim2\": 124,\n",
        "    \"nn_lr\": 0.0013365869940667667,\n",
        "    \"nn_batch_size\": 64,\n",
        "    \"nn_epochs\": 21\n",
        "}\n",
        "\n",
        "# Define Neural Network class using best hyperparameters\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, best_trial_params[\"nn_hidden_dim1\"])\n",
        "        self.fc2 = nn.Linear(best_trial_params[\"nn_hidden_dim1\"], best_trial_params[\"nn_hidden_dim2\"])\n",
        "        self.fc3 = nn.Linear(best_trial_params[\"nn_hidden_dim2\"], 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Ensemble weights (from your previous tuning; adjust if needed)\n",
        "weight_cat = 0.6742\n",
        "weight_lgb = 0.2760\n",
        "weight_nn  = 0.0498\n",
        "\n",
        "final_predictions = {}      # To store final predicted 5-day return per stock\n",
        "predicted_prices = {}       # To store final predicted price per stock\n",
        "evaluation_metrics = {}     # To store evaluation metrics per stock\n",
        "\n",
        "# Loop over each stock in the test dictionary\n",
        "for stock in X_test_dict.keys():\n",
        "    print(f\"\\nProcessing stock: {stock}\")\n",
        "\n",
        "    # Get training and test data for this stock\n",
        "    X_train_stock = X_train_dict[stock]\n",
        "    y_train_stock = y_train_dict[stock]\n",
        "    X_test_stock  = X_test_dict[stock]\n",
        "    y_test_stock  = y_test_dict[stock]\n",
        "\n",
        "    input_dim = X_train_stock.shape[1]\n",
        "\n",
        "    # --- Train CatBoost for this stock ---\n",
        "    cat_model = CatBoostRegressor(\n",
        "        iterations=best_trial_params[\"catboost_iterations\"],\n",
        "        learning_rate=best_trial_params[\"catboost_lr\"],\n",
        "        depth=best_trial_params[\"catboost_depth\"],\n",
        "        l2_leaf_reg=best_trial_params[\"catboost_l2_leaf_reg\"],\n",
        "        loss_function=\"RMSE\",\n",
        "        verbose=0\n",
        "    )\n",
        "    cat_model.fit(X_train_stock, y_train_stock)\n",
        "\n",
        "    # --- Train LightGBM for this stock ---\n",
        "    lgb_model = lgb.LGBMRegressor(\n",
        "        n_estimators=best_trial_params[\"lgbm_n_estimators\"],\n",
        "        learning_rate=best_trial_params[\"lgbm_lr\"],\n",
        "        num_leaves=best_trial_params[\"lgbm_num_leaves\"],\n",
        "        feature_fraction=best_trial_params[\"lgbm_feature_fraction\"],\n",
        "        subsample=best_trial_params[\"lgbm_subsample\"]\n",
        "    )\n",
        "    lgb_model.fit(X_train_stock, y_train_stock)\n",
        "\n",
        "    # --- Train Neural Network for this stock ---\n",
        "    nn_model = NeuralNet(input_dim)\n",
        "    optimizer = optim.Adam(nn_model.parameters(), lr=best_trial_params[\"nn_lr\"])\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Convert training data to tensors\n",
        "    X_train_tensor = torch.tensor(X_train_stock, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train_stock, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    nn_model.train()\n",
        "    for epoch in range(best_trial_params[\"nn_epochs\"]):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = nn_model(X_train_tensor)\n",
        "        loss = criterion(outputs, y_train_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # --- Predictions on test set for this stock ---\n",
        "    cat_preds = cat_model.predict(X_test_stock)         # numpy array\n",
        "    lgb_preds = lgb_model.predict(X_test_stock)           # numpy array\n",
        "    nn_model.eval()\n",
        "    nn_preds = nn_model(torch.tensor(X_test_stock, dtype=torch.float32)).detach().numpy().flatten()\n",
        "\n",
        "    # Weighted ensemble predictions for each test sample\n",
        "    ensemble_preds = (cat_preds * weight_cat) + (lgb_preds * weight_lgb) + (nn_preds * weight_nn)\n",
        "\n",
        "    # Use the last sample's prediction as the forecasted 5-day return\n",
        "    final_return = ensemble_preds[-1]\n",
        "    final_predictions[stock] = final_return\n",
        "\n",
        "    # Get the last known price for this stock from processed_df\n",
        "    last_price = processed_df[processed_df['Stock'] == stock]['Last Price'].iloc[-1]\n",
        "    predicted_price = last_price * (1 + final_return)\n",
        "    predicted_prices[stock] = predicted_price\n",
        "\n",
        "    # --- Evaluate predictions on test set ---\n",
        "    mae = mean_absolute_error(y_test_stock, ensemble_preds)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_stock, ensemble_preds))\n",
        "    r2 = r2_score(y_test_stock, ensemble_preds)\n",
        "    evaluation_metrics[stock] = {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
        "\n",
        "# --- Print final predictions and evaluation metrics ---\n",
        "print(\"\\nFinal Predictions (Using Best Hyperparameters from Tuning):\")\n",
        "for stock in final_predictions.keys():\n",
        "    print(f\"{stock}: Predicted 5-day return = {final_predictions[stock]*100:.2f}%, Predicted price = ${predicted_prices[stock]:.2f}\")\n",
        "\n",
        "print(\"\\nEvaluation Metrics on Test Sets:\")\n",
        "for stock, metrics in evaluation_metrics.items():\n",
        "    print(f\"{stock}: MAE = {metrics['MAE']:.4f}, RMSE = {metrics['RMSE']:.4f}, R2 = {metrics['R2']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du795mZyFwNw",
        "outputId": "8ab0cc9e-7c9e-4565-99d2-69ef8ea892f8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing stock: ALT US Equity\n",
            "\n",
            "Processing stock: CELH US Equity\n",
            "\n",
            "Processing stock: CVNA US Equity\n",
            "\n",
            "Processing stock: FUBO US Equity\n",
            "\n",
            "Processing stock: UPST US Equity\n",
            "\n",
            "Final Predictions (Using Best Hyperparameters from Tuning):\n",
            "ALT US Equity: Predicted 5-day return = 9.28%, Predicted price = $7.03\n",
            "CELH US Equity: Predicted 5-day return = -4.93%, Predicted price = $31.01\n",
            "CVNA US Equity: Predicted 5-day return = 7.94%, Predicted price = $241.02\n",
            "FUBO US Equity: Predicted 5-day return = -7.20%, Predicted price = $3.49\n",
            "UPST US Equity: Predicted 5-day return = -0.59%, Predicted price = $71.35\n",
            "\n",
            "Evaluation Metrics on Test Sets:\n",
            "ALT US Equity: MAE = 0.0963, RMSE = 0.1016, R2 = -3.4892\n",
            "CELH US Equity: MAE = 0.1229, RMSE = 0.1471, R2 = -0.3019\n",
            "CVNA US Equity: MAE = 0.0743, RMSE = 0.0963, R2 = -1.1688\n",
            "FUBO US Equity: MAE = 0.1069, RMSE = 0.1221, R2 = -1.1256\n",
            "UPST US Equity: MAE = 0.0939, RMSE = 0.1345, R2 = -0.0545\n"
          ]
        }
      ]
    }
  ]
}